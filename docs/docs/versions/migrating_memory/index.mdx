---
sidebar_position: 1
---

# How to migrate from v0.0 memory

The concept of memory has evolved significantly in LangChain since its initial release.

This guide will help you migrate your usage of memory implementations from LangChain v0.0.x
to the new powerful persistence implementations of LangGraph.

:::info How deprecated implementations work
Even though many of these implementations are deprecated, they are **still supported** in the codebase.
However, they are not recommended for new development, and we recommend re-implementing them using the following guides!

To see the planned removal version for each deprecated implementation, check their API reference.
:::

:::info Prerequisites

These guides assume some familiarity with the following concepts:
- [LangGraph](https://langchain-ai.github.io/langgraph/)
- [v0.0.x Memory](https://python.langchain.com/v0.1/docs/modules/memory/)
- [How to add persistence ("memory") to your graph](https://langchain-ai.github.io/langgraph/how-tos/persistence/)
:::

LangChain maintains a number of legacy abstractions. Many of these can be reimplemented via short combinations modern LangChain and LangGraph primitives.

### Advantages

Some advantages to using LangGraph persistence instead of existing v0.0.x memory implementations include:

- Built-in support for multi-user, multi-conversation scenarios which is often a requirement for real-world conversational AI applications.
- Ability to save and resume complex state at any time for error recovery, human-in-the-loop workflows, time travel interactions, and more.
- Full support for both LLM and chat models. In contrast, the v0.0.x memory abstractions were created prior to the existence and widespread adoption of chat model APIs, and so it does not work well with chat models (e.g., fails with tool calling chat models).
- Offers a high degree of customization and control over the memory implementation, including the ability to use different backend.

### Migrations

Basic memory implementation that simply stores the conversation history:

- [ConversationBufferMemory](./ConversationBufferMemory.ipynb): A basic memory implementation that simply stores the conversation history.
- [ConversationStringBufferMemory](./ConversationStringBufferMemory.ipynb): A special case of `ConversationBufferMemory` designed for LLMs and no longer relevant. Please follow the guide for [ConversationBufferMemory](./ConversationBufferMemory.ipynb).

The following memory implementations are variations on top of `ConversationBufferMemory`. The implementations do some additional processing to the conversation history with the goal of keeping the conversation within the limits of the context window of the chat model.

- ConversationBufferWindowMemory: Keeps the last `n` turns of the conversation. Drops the oldest turn when the buffer is full.
- ConversationTokenBufferMemory: Keeps only the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit.
- ConversationSummaryMemory: Continually summarizes the conversation history. The summary is updated after each conversation turn. The abstraction returns the summary of the conversation history.
- ConversationSummaryBufferMemory: Provides a running summary of the conversation together with the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit. However, the original implementation was not correct, the memory implementation returns a running summary together with all the messages in the conversation (and did not apply the constraint).


Other memory implementations that add some additional functionality:

- CombinedMemory - This abstraction accepted a list of `BaseMemory` and fetched relevant memory information from each of them based on the input.
- SimpleMemory - This was used to add read-only hard-coded context. Users can simply write this information into the prompt.
- ReadOnlySharedMemory - Provided a read only view of an existing `BaseMemory` implementation.
- VectorStoreRetrieverMemory

Entity stores:

- BaseEntityStore
- InMemoryEntityStore
- ConversationEntityMemory
- RedisEntityStore: This is a specific implementation of `BaseEntityStore` that uses Redis as the backend.
- SQLiteEntityStore: This is a specific implementation of `BaseEntityStore` that uses SQLite as the backend.
- UpstashRedisEntityStore: This is a specific implementation of `BaseEntityStore` that uses Upstash as the backend.

Custom code based on existing abstractions:

If you have custom code that sub-classes from `BaseMemory`, `BaseChatMemory`, or `BaseEntityStore`, you will need to re-implement the code using the appropriate primitives. Often, this involves lightweight processing of the conversation history, such as filtering or summarization.


Check out [LangGraph docs](https://langchain-ai.github.io/langgraph/) for more background information.
