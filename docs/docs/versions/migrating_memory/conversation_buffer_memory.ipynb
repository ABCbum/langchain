{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce8457ed-c0b1-4a74-abbd-9d3d2211270f",
   "metadata": {},
   "source": [
    "# Migrating from ConversationBufferMemory\n",
    "\n",
    "[ConversationBufferMemory](https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer.ConversationBufferMemory.html) was used to keep track of a conversation between a human and an ai asstistant.\n",
    "\n",
    "The `ConversationBufferMemory` implementation was limited and only worked for:\n",
    "\n",
    "1. LLMs (i.e., non chat models)\n",
    "2. Chat Models without native tool calling capability.\n",
    "\n",
    "We recommenbd that users swap to [persistence using langgraph](https://langchain-ai.github.io/langgraph/how-tos/persistence/).\n",
    "\n",
    "Some advantages to using LangGraph persistence:\n",
    "\n",
    "- LangGraph persistence is much more powerful than simple chat memory -- it lets you save and resume complex state at any time for error recovery, human-in-the-loop workflows, time travel interactions, and more.\n",
    "- LangGraph persistence is built to support multi user, multi thread scenarios and will work well in production settings.\n",
    "- Both LLMs and chat models are fully supported.\n",
    "- Users have granular control over how memory is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b99b47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --upgrade --quiet langchain-openai langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "717c8673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3621b62-a037-42b8-8faa-59575608bb8b",
   "metadata": {},
   "source": [
    "## LLMChain / ConversationChain\n",
    "\n",
    "### Legacy\n",
    "\n",
    "Below is example usage of `ConversationBufferMemory` with an `LLMChain` or an equivalent `ConversationChain`.\n",
    "\n",
    "<details open>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b6e1063-cf3a-456a-bf7d-830e5c1d2864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_925844/1840646419.py:23: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  legacy_chain = LLMChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Nice to meet you, Bob! How can I assist you today?', 'chat_history': [HumanMessage(content='my name is bob', additional_kwargs={}, response_metadata={}), AIMessage(content='Nice to meet you, Bob! How can I assist you today?', additional_kwargs={}, response_metadata={})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "])\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", \n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "legacy_chain = LLMChain(\n",
    "    llm=ChatOpenAI(), prompt=prompt, memory=memory)\n",
    "\n",
    "legacy_result = legacy_chain.invoke({\"text\": \"my name is bob\"})\n",
    "print(legacy_result)\n",
    "\n",
    "legacy_result = legacy_chain.invoke({\"text\": \"what was my name\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89e97b",
   "metadata": {},
   "source": [
    "Note that `LLMChain` by default returned a `dict` containing both the input and the output from `StrOutputParser`, so to extract the output, you need to access the `\"text\"` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7fa1618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Bob. How can I assist you further, Bob?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legacy_result[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3b527-c09e-4c77-9711-c3cc4506cd95",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### LangGraph\n",
    "\n",
    "<details open>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e591965c-c4d7-4df7-966d-4d14bd46e157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm bob\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Bob! How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what was my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob. Is there anything else you would like to know or talk about?\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, MessagesState\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "memory = MemorySaver() # Add in persistence\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Let's test out application\n",
    "# The thread_id will be a key that will be used to identify\n",
    "# this particular conversation.\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "input_message = HumanMessage(content=\"hi! I'm bob\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# Here, let's confirm that the AI remembers our name!\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "input_message = HumanMessage(content=\"what was my name?\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a19fdf6-f6c4-43bc-94f8-4b521b3ea3b6",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Agent Executor\n",
    "\n",
    "This shows usage of an Agent Executor with an agent constructed using [create_tool_calling_agent](https://api.python.langchain.com/en/latest/agents/langchain.agents.tool_calling_agent.base.create_tool_calling_agent.html) function.\n",
    "\n",
    "The same logic applies to idea applies to other pre-built agents which can be the pre-built [create_react_agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/).\n",
    "\n",
    "### Legacy Usage\n",
    "\n",
    "<details open>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc2928de-d7a4-4f87-ab96-59bde9a3829f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eugene/.pyenv/versions/3.11.4/envs/langgraph_3_11/lib/python3.11/site-packages/langsmith/client.py:5301: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  prompt = loads(json.dumps(prompt_object.manifest))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'hi! my name is bob what is my age?', 'chat_history': [HumanMessage(content='hi! my name is bob what is my age?', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Bob! You are 42 years old. If you need any more assistance or information, feel free to ask!', additional_kwargs={}, response_metadata={})], 'output': 'Hello Bob! You are 42 years old. If you need any more assistance or information, feel free to ask!'}\n",
      "\n",
      "{'input': 'do you remember my name?', 'chat_history': [HumanMessage(content='hi! my name is bob what is my age?', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Bob! You are 42 years old. If you need any more assistance or information, feel free to ask!', additional_kwargs={}, response_metadata={}), HumanMessage(content='do you remember my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Yes, your name is Bob. How can I assist you further, Bob?', additional_kwargs={}, response_metadata={})], 'output': 'Yes, your name is Bob. How can I assist you further, Bob?'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_core.tools import tool\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "@tool\n",
    "def get_user_age(name: str) -> str:\n",
    "    \"\"\"Use this tool to find the user's age.\"\"\"\n",
    "    # This is a placeholder for the actual implementation\n",
    "    if 'bob' in name.lower():\n",
    "        return '42 years old'\n",
    "    return '41 years old'\n",
    "\n",
    "tools = [get_user_age]\n",
    "\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "# Instantiate memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", \n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Create an agent\n",
    "agent = create_tool_calling_agent(model, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory)\n",
    "\n",
    "# Verify that the agent can use tools\n",
    "print(agent_executor.invoke({\"input\": \"hi! my name is bob what is my age?\"}))\n",
    "print()\n",
    "# Verify that the agent has access to conversation history.\n",
    "# The agent should be able to answer that the user's name is bob.\n",
    "print(agent_executor.invoke({\"input\": \"do you remember my name?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4866ae9-e683-44dc-a77b-da1737d3a645",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### LangGraph\n",
    "\n",
    "If you want to use a pre-built agent, where you only supply the tools and the prompt, you can follow [this guide](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-memory/).\n",
    "\n",
    "<details open>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdb29c9b-bc57-4512-9430-c5d5e3f91e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm bob. What is my age?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_user_age (call_E4SDRy9Hgi5t9Vt5g73Rxjst)\n",
      " Call ID: call_E4SDRy9Hgi5t9Vt5g73Rxjst\n",
      "  Args:\n",
      "    name: bob\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_user_age\n",
      "\n",
      "42 years old\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Bob, you are 42 years old.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "do you remember my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, your name is Bob.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_user_age(name: str) -> str:\n",
    "    \"\"\"Use this tool to find the user's age.\"\"\"\n",
    "    # This is a placeholder for the actual implementation\n",
    "    if 'bob' in name.lower():\n",
    "        return '42 years old'\n",
    "    return '41 years old'\n",
    "\n",
    "memory = MemorySaver()\n",
    "model = ChatOpenAI()\n",
    "app = create_react_agent(model, tools=[get_user_age], checkpointer=memory)\n",
    "\n",
    "# Let's test out application\n",
    "# The thread_id will be a key that will be used to identify\n",
    "# this particular conversation.\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Tell the AI that our name is Bob, and ask it to use a tool to confirm\n",
    "# that it's capable of working like an agent.\n",
    "input_message = HumanMessage(content=\"hi! I'm bob. What is my age?\")\n",
    "\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# Confirm that the chat bot has access to previous conversation\n",
    "# and can respond to the user saying that the user's name is Bob.\n",
    "input_message = HumanMessage(content=\"do you remember my name?\")\n",
    "\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe63e424-1111-4f6a-a9c9-0887eb150ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm bob. What is my age?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_user_age (call_IkvTlGo0Jgy0JRSU7cpculkl)\n",
      " Call ID: call_IkvTlGo0Jgy0JRSU7cpculkl\n",
      "  Args:\n",
      "    name: Bob\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_user_age\n",
      "\n",
      "42 years old\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Bob! You are 42 years old. If you need any more information or assistance, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# Now, let's confirm that if we use a different thread\n",
    "# that the bot will not know what our name is since\n",
    "# it's a new conversation!\n",
    "config = {\"configurable\": {\"thread_id\": \"123456789\"}}\n",
    "\n",
    "input_message = HumanMessage(content=\"hi! I'm bob. What is my age?\")\n",
    "\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1d693-e6ed-4a8a-bcdd-a685007eafe5",
   "metadata": {},
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2717810",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "* [LangGraph quickstart tutorial](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\n",
    "* [How to add persistence (\"memory\") to your graph](https://langchain-ai.github.io/langgraph/how-tos/persistence/)\n",
    "* [How to manage conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/)\n",
    "* [How to add summary of the conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
