{
 "cells": [
  {
   "cell_type": "raw",
   "id": "018f3868-e60d-4db6-a1c6-c6633c66b1f4",
   "metadata": {},
   "source": [
    "---\n",
    "keywords: [LCEL, fallbacks]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c9cbd6",
   "metadata": {},
   "source": [
    "# How to add fallbacks to a runnable\n",
    "\n",
    "When working with language models, you may often encounter issues from the underlying APIs, whether these be rate limiting or downtime. Therefore, as you go to move your LLM applications into production it becomes more and more important to safeguard against these. That's why we've introduced the concept of fallbacks. \n",
    "\n",
    "A **fallback** is an alternative plan that may be used in an emergency.\n",
    "\n",
    "Crucially, fallbacks can be applied not only on the LLM level but on the whole runnable level. This is important because often times different models require different prompts. So if your call to OpenAI fails, you don't just want to send the same prompt to Anthropic - you probably want to use a different prompt template and send a different version there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb9ba9",
   "metadata": {},
   "source": [
    "## Fallback for LLM API Errors\n",
    "\n",
    "This is maybe the most common use case for fallbacks. A request to an LLM API can fail for a variety of reasons - the API could be down, you could have hit rate limits, any number of things. Therefore, using fallbacks can help protect against these types of things.\n",
    "\n",
    "IMPORTANT: By default, a lot of the LLM wrappers catch errors and retry. You will most likely want to turn those off when working with fallbacks. Otherwise the first wrapper will keep on retrying and not failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a449a2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:50.904162Z",
     "iopub.status.busy": "2024-09-11T18:18:50.903833Z",
     "iopub.status.idle": "2024-09-11T18:18:52.376410Z",
     "shell.execute_reply": "2024-09-11T18:18:52.375919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e893bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:52.379177Z",
     "iopub.status.busy": "2024-09-11T18:18:52.378979Z",
     "iopub.status.idle": "2024-09-11T18:18:52.893628Z",
     "shell.execute_reply": "2024-09-11T18:18:52.893379Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4847c82d",
   "metadata": {},
   "source": [
    "First, let's mock out what happens if we hit a RateLimitError from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfdd8bf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:52.895220Z",
     "iopub.status.busy": "2024-09-11T18:18:52.895092Z",
     "iopub.status.idle": "2024-09-11T18:18:52.901800Z",
     "shell.execute_reply": "2024-09-11T18:18:52.901577Z"
    }
   },
   "outputs": [],
   "source": [
    "from unittest.mock import patch\n",
    "\n",
    "import httpx\n",
    "from openai import RateLimitError\n",
    "\n",
    "request = httpx.Request(\"GET\", \"/\")\n",
    "response = httpx.Response(200, request=request)\n",
    "error = RateLimitError(\"rate limit\", response=response, body=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6fdffc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:52.903147Z",
     "iopub.status.busy": "2024-09-11T18:18:52.903057Z",
     "iopub.status.idle": "2024-09-11T18:18:52.927770Z",
     "shell.execute_reply": "2024-09-11T18:18:52.927490Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note that we set max_retries = 0 to avoid retrying on RateLimits, etc\n",
    "openai_llm = ChatOpenAI(model=\"gpt-4o-mini\", max_retries=0)\n",
    "anthropic_llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "llm = openai_llm.with_fallbacks([anthropic_llm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "584461ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:52.929244Z",
     "iopub.status.busy": "2024-09-11T18:18:52.929172Z",
     "iopub.status.idle": "2024-09-11T18:18:52.941248Z",
     "shell.execute_reply": "2024-09-11T18:18:52.941026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit error\n"
     ]
    }
   ],
   "source": [
    "# Let's use just the OpenAI LLm first, to show that we run into an error\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        print(openai_llm.invoke(\"Why did the chicken cross the road?\"))\n",
    "    except RateLimitError:\n",
    "        print(\"Hit error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fc1e673",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:52.942503Z",
     "iopub.status.busy": "2024-09-11T18:18:52.942413Z",
     "iopub.status.idle": "2024-09-11T18:18:55.175422Z",
     "shell.execute_reply": "2024-09-11T18:18:55.174328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"There are many joking responses to this classic riddle, but the basic premise is that the chicken crossed the road for some funny or unexpected reason. Some common punchlines include:\\n\\n- To get to the other side.\\n- I don't know, why did the chicken cross the road?\\n- To prove to the possum it could be done.\\n- To show the armadillo it could be done.\\n- Because it was the chicken's day off.\\n- To fetch a pail of water.\\n\\nThe simplicity and ambiguity of the question is what makes this riddle so enduring and open to humorous interpretations. The actual reason why the chicken crossed the road is left up to the imagination of the person hearing the joke.\" additional_kwargs={} response_metadata={'id': 'msg_01Eh8JuxtpMkeYMPQn4Pz52b', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 15, 'output_tokens': 161}} id='run-e2bb674c-6c23-41f5-b572-7f4ae8d03d9c-0' usage_metadata={'input_tokens': 15, 'output_tokens': 161, 'total_tokens': 176}\n"
     ]
    }
   ],
   "source": [
    "# Now let's try with fallbacks to Anthropic\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        print(llm.invoke(\"Why did the chicken cross the road?\"))\n",
    "    except RateLimitError:\n",
    "        print(\"Hit error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00bea25",
   "metadata": {},
   "source": [
    "We can use our \"LLM with Fallbacks\" as we would a normal LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f8eaaa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:55.180608Z",
     "iopub.status.busy": "2024-09-11T18:18:55.180165Z",
     "iopub.status.idle": "2024-09-11T18:18:56.614974Z",
     "shell.execute_reply": "2024-09-11T18:18:56.614215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I don't actually know why the kangaroo crossed the road. Jokes and riddles about animals crossing roads are often just silly little puzzles without a real punchline. But I'm happy to engage with your sense of humor! You have a fun and creative way of thinking up these kinds of lighthearted quips. I enjoy our playful exchanges and think you're very clever.\" additional_kwargs={} response_metadata={'id': 'msg_01KEc7euBvTkMkQmZanXnXdw', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 30, 'output_tokens': 85}} id='run-6160f8a3-080d-4166-ab7a-4128f1de2da4-0' usage_metadata={'input_tokens': 30, 'output_tokens': 85, 'total_tokens': 115}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're a nice assistant who always includes a compliment in your response\",\n",
    "        ),\n",
    "        (\"human\", \"Why did the {animal} cross the road\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | llm\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        print(chain.invoke({\"animal\": \"kangaroo\"}))\n",
    "    except RateLimitError:\n",
    "        print(\"Hit error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d62241b",
   "metadata": {},
   "source": [
    "## Fallback for Sequences\n",
    "\n",
    "We can also create fallbacks for sequences, that are sequences themselves. Here we do that with two different models: ChatOpenAI and then normal OpenAI (which does not use a chat model). Because OpenAI is NOT a chat model, you likely want a different prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d0b8056",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:56.619501Z",
     "iopub.status.busy": "2024-09-11T18:18:56.618801Z",
     "iopub.status.idle": "2024-09-11T18:18:56.651403Z",
     "shell.execute_reply": "2024-09-11T18:18:56.650895Z"
    }
   },
   "outputs": [],
   "source": [
    "# First let's create a chain with a ChatModel\n",
    "# We add in a string output parser here so the outputs between the two are the same type\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're a nice assistant who always includes a compliment in your response\",\n",
    "        ),\n",
    "        (\"human\", \"Why did the {animal} cross the road\"),\n",
    "    ]\n",
    ")\n",
    "# Here we're going to use a bad model name to easily create a chain that will error\n",
    "chat_model = ChatOpenAI(model=\"gpt-fake\")\n",
    "bad_chain = chat_prompt | chat_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d1fc2a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:56.654480Z",
     "iopub.status.busy": "2024-09-11T18:18:56.654270Z",
     "iopub.status.idle": "2024-09-11T18:18:56.673870Z",
     "shell.execute_reply": "2024-09-11T18:18:56.673429Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now lets create a chain with the normal OpenAI model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "prompt_template = \"\"\"Instructions: You should always include a compliment in your response.\n",
    "\n",
    "Question: Why did the {animal} cross the road?\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "llm = OpenAI()\n",
    "good_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "283bfa44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:56.676574Z",
     "iopub.status.busy": "2024-09-11T18:18:56.676321Z",
     "iopub.status.idle": "2024-09-11T18:18:57.706000Z",
     "shell.execute_reply": "2024-09-11T18:18:57.705373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nResponse: The turtle must have been very determined to cross the road!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now create a final chain which combines the two\n",
    "chain = bad_chain.with_fallbacks([good_chain])\n",
    "chain.invoke({\"animal\": \"turtle\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4685b4",
   "metadata": {},
   "source": [
    "## Fallback for Long Inputs\n",
    "\n",
    "One of the big limiting factors of LLMs is their context window. Usually, you can count and track the length of prompts before sending them to an LLM, but in situations where that is hard/complicated, you can fallback to a model with a longer context length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "564b84c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:57.708723Z",
     "iopub.status.busy": "2024-09-11T18:18:57.708528Z",
     "iopub.status.idle": "2024-09-11T18:18:57.742702Z",
     "shell.execute_reply": "2024-09-11T18:18:57.742341Z"
    }
   },
   "outputs": [],
   "source": [
    "short_llm = ChatOpenAI()\n",
    "long_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "llm = short_llm.with_fallbacks([long_llm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e27a775",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:57.745164Z",
     "iopub.status.busy": "2024-09-11T18:18:57.745028Z",
     "iopub.status.idle": "2024-09-11T18:18:57.747351Z",
     "shell.execute_reply": "2024-09-11T18:18:57.747050Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = \"What is the next number: \" + \", \".join([\"one\", \"two\"] * 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a502731",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:57.749036Z",
     "iopub.status.busy": "2024-09-11T18:18:57.748916Z",
     "iopub.status.idle": "2024-09-11T18:18:58.154260Z",
     "shell.execute_reply": "2024-09-11T18:18:58.153591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"Sorry! We've encountered an issue with repetitive patterns in your prompt. Please try again with a different prompt.\", 'type': 'invalid_request_error', 'param': 'prompt', 'code': 'invalid_prompt'}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(short_llm.invoke(inputs))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d91ba5d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:58.160501Z",
     "iopub.status.busy": "2024-09-11T18:18:58.160157Z",
     "iopub.status.idle": "2024-09-11T18:18:59.336076Z",
     "shell.execute_reply": "2024-09-11T18:18:59.335476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"Sorry! We've encountered an issue with repetitive patterns in your prompt. Please try again with a different prompt.\", 'type': 'invalid_request_error', 'param': 'prompt', 'code': 'invalid_prompt'}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(llm.invoke(inputs))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6735df",
   "metadata": {},
   "source": [
    "## Fallback to Better Model\n",
    "\n",
    "Often times we ask models to output format in a specific format (like JSON). Models like GPT-3.5 can do this okay, but sometimes struggle. This naturally points to fallbacks - we can try with GPT-3.5 (faster, cheaper), but then if parsing fails we can use GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "867a3793",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:59.339663Z",
     "iopub.status.busy": "2024-09-11T18:18:59.339393Z",
     "iopub.status.idle": "2024-09-11T18:18:59.386131Z",
     "shell.execute_reply": "2024-09-11T18:18:59.385731Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8d9959d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:59.388573Z",
     "iopub.status.busy": "2024-09-11T18:18:59.388404Z",
     "iopub.status.idle": "2024-09-11T18:18:59.391565Z",
     "shell.execute_reply": "2024-09-11T18:18:59.391089Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"what time was {event} (in %Y-%m-%dT%H:%M:%S.%fZ format - only return this value)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98087a76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:59.393983Z",
     "iopub.status.busy": "2024-09-11T18:18:59.393605Z",
     "iopub.status.idle": "2024-09-11T18:18:59.420328Z",
     "shell.execute_reply": "2024-09-11T18:18:59.419980Z"
    }
   },
   "outputs": [],
   "source": [
    "# In this case we are going to do the fallbacks on the LLM + output parser level\n",
    "# Because the error will get raised in the OutputParser\n",
    "openai_35 = ChatOpenAI() | DatetimeOutputParser()\n",
    "openai_4 = ChatOpenAI(model=\"gpt-4\") | DatetimeOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17ec9e8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:59.422464Z",
     "iopub.status.busy": "2024-09-11T18:18:59.422237Z",
     "iopub.status.idle": "2024-09-11T18:18:59.424253Z",
     "shell.execute_reply": "2024-09-11T18:18:59.424012Z"
    }
   },
   "outputs": [],
   "source": [
    "only_35 = prompt | openai_35\n",
    "fallback_4 = prompt | openai_35.with_fallbacks([openai_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e536f0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:59.425613Z",
     "iopub.status.busy": "2024-09-11T18:18:59.425523Z",
     "iopub.status.idle": "2024-09-11T18:19:00.312278Z",
     "shell.execute_reply": "2024-09-11T18:19:00.311756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1994-01-30 18:30:00\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(only_35.invoke({\"event\": \"the superbowl in 1994\"}))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01355c5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:19:00.316207Z",
     "iopub.status.busy": "2024-09-11T18:19:00.315949Z",
     "iopub.status.idle": "2024-09-11T18:19:00.939959Z",
     "shell.execute_reply": "2024-09-11T18:19:00.939461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1994-01-30 18:30:00\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(fallback_4.invoke({\"event\": \"the superbowl in 1994\"}))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c537f9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
