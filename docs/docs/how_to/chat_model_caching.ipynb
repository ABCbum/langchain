{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcf87b32",
   "metadata": {},
   "source": [
    "# How to cache chat model responses\n",
    "\n",
    ":::info Prerequisites\n",
    "\n",
    "This guide assumes familiarity with the following concepts:\n",
    "- [Chat models](/docs/concepts/#chat-models)\n",
    "- [LLMs](/docs/concepts/#llms)\n",
    "\n",
    ":::\n",
    "\n",
    "LangChain provides an optional caching layer for chat models. This is useful for two main reasons:\n",
    "\n",
    "- It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times. This is especially useful during app development.\n",
    "- It can speed up your application by reducing the number of API calls you make to the LLM provider.\n",
    "\n",
    "This guide will walk you through how to enable this in your apps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289b31de",
   "metadata": {},
   "source": [
    "```{=mdx}\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs customVarName=\"llm\" />\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6641f37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:07:23.124798Z",
     "iopub.status.busy": "2024-09-11T18:07:23.124578Z",
     "iopub.status.idle": "2024-09-11T18:07:23.765564Z",
     "shell.execute_reply": "2024-09-11T18:07:23.765295Z"
    }
   },
   "outputs": [
    {
     "ename": "StdinNotImplementedError",
     "evalue": "getpass was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgetpass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getpass\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m----> 9\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mgetpass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOpenAI()\n",
      "File \u001b[0;32m~/langchain/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py:1256\u001b[0m, in \u001b[0;36mKernel.getpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1255\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetpass was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1258\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m: getpass was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass()\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5472a032",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:07:23.767015Z",
     "iopub.status.busy": "2024-09-11T18:07:23.766906Z",
     "iopub.status.idle": "2024-09-11T18:07:23.768534Z",
     "shell.execute_reply": "2024-09-11T18:07:23.768307Z"
    }
   },
   "outputs": [],
   "source": [
    "# <!-- ruff: noqa: F821 -->\n",
    "from langchain_core.globals import set_llm_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357b89a8",
   "metadata": {},
   "source": [
    "## In Memory Cache\n",
    "\n",
    "This is an ephemeral cache that stores model calls in memory. It will be wiped when your environment restarts, and is not shared across processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "113e719a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:07:23.769825Z",
     "iopub.status.busy": "2024-09-11T18:07:23.769745Z",
     "iopub.status.idle": "2024-09-11T18:07:23.787797Z",
     "shell.execute_reply": "2024-09-11T18:07:23.787562Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:6\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_core.caches import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2121434",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:07:23.789052Z",
     "iopub.status.busy": "2024-09-11T18:07:23.788978Z",
     "iopub.status.idle": "2024-09-11T18:07:23.793115Z",
     "shell.execute_reply": "2024-09-11T18:07:23.792920Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:2\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The second time it is, so it goes faster\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88ff8af",
   "metadata": {},
   "source": [
    "## SQLite Cache\n",
    "\n",
    "This cache implementation uses a `SQLite` database to store responses, and will last across process restarts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99290ab4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:07:23.794456Z",
     "iopub.status.busy": "2024-09-11T18:07:23.794373Z",
     "iopub.status.idle": "2024-09-11T18:07:23.920849Z",
     "shell.execute_reply": "2024-09-11T18:07:23.920370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: .langchain.db: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm .langchain.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe826c5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:07:23.923523Z",
     "iopub.status.busy": "2024-09-11T18:07:23.923321Z",
     "iopub.status.idle": "2024-09-11T18:07:26.765840Z",
     "shell.execute_reply": "2024-09-11T18:07:26.765229Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can do the same thing with a SQLite cache\n",
    "from langchain_community.cache import SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb558734",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:07:26.769988Z",
     "iopub.status.busy": "2024-09-11T18:07:26.769429Z",
     "iopub.status.idle": "2024-09-11T18:07:26.800947Z",
     "shell.execute_reply": "2024-09-11T18:07:26.799202Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:2\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "497c7000",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:07:26.809534Z",
     "iopub.status.busy": "2024-09-11T18:07:26.807911Z",
     "iopub.status.idle": "2024-09-11T18:07:26.843198Z",
     "shell.execute_reply": "2024-09-11T18:07:26.839484Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:2\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The second time it is, so it goes faster\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2950a913",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "You've now learned how to cache model responses to save time and money.\n",
    "\n",
    "Next, check out the other how-to guides chat models in this section, like [how to get a model to return structured output](/docs/how_to/structured_output) or [how to create your own custom chat model](/docs/how_to/custom_chat_model)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
