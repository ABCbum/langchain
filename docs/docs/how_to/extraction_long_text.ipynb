{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e161a8a-fcf0-4d55-933e-da271ce28d7e",
   "metadata": {},
   "source": [
    "# How to handle long text when doing extraction\n",
    "\n",
    "When working with files, like PDFs, you're likely to encounter text that exceeds your language model's context window. To process this text, consider these strategies:\n",
    "\n",
    "1. **Change LLM** Choose a different LLM that supports a larger context window.\n",
    "2. **Brute Force** Chunk the document, and extract content from each chunk.\n",
    "3. **RAG** Chunk the document, index the chunks, and only extract content from a subset of chunks that look \"relevant\".\n",
    "\n",
    "Keep in mind that these strategies have different trade off and the best strategy likely depends on the application that you're designing!\n",
    "\n",
    "This guide demonstrates how to implement strategies 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57969139-ad0a-487e-97d8-cb30e2af9742",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we'll install the dependencies needed for this guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3b4d838-5be4-4207-8a4a-9ef5624c48f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:09.050589Z",
     "iopub.status.busy": "2024-09-11T18:18:09.050147Z",
     "iopub.status.idle": "2024-09-11T18:18:17.776051Z",
     "shell.execute_reply": "2024-09-11T18:18:17.775553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "langchain-benchmarks 0.0.14 requires langchain<0.3.0,>=0.2.7, but you have langchain 0.3.0.dev1 which is incompatible.\r\n",
      "langchain-benchmarks 0.0.14 requires langchain-community<0.3,>=0.2, but you have langchain-community 0.3.0.dev1 which is incompatible.\r\n",
      "langchain-benchmarks 0.0.14 requires langchain-openai<0.2.0,>=0.1.14, but you have langchain-openai 0.2.0.dev2 which is incompatible.\r\n",
      "langchain-aws 0.1.15 requires langchain-core<0.3,>=0.2.29, but you have langchain-core 0.3.0.dev4 which is incompatible.\r\n",
      "langchain-chroma 0.1.3 requires langchain-core<0.3,>=0.1.40, but you have langchain-core 0.3.0.dev4 which is incompatible.\r\n",
      "langchain-together 0.1.5 requires langchain-core<0.3.0,>=0.2.26, but you have langchain-core 0.3.0.dev4 which is incompatible.\r\n",
      "langchain-together 0.1.5 requires langchain-openai<0.2.0,>=0.1.16, but you have langchain-openai 0.2.0.dev2 which is incompatible.\r\n",
      "langserve 0.2.2 requires langchain-core<0.3,>=0.1, but you have langchain-core 0.3.0.dev4 which is incompatible.\r\n",
      "langchain-standard-tests 0.1.1 requires langchain-core<0.3,>=0.1.40, but you have langchain-core 0.3.0.dev4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-community lxml faiss-cpu langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac000b03-33fc-414f-8f2c-3850df621a35",
   "metadata": {},
   "source": [
    "Now we need some example data! Let's download an article about [cars from wikipedia](https://en.wikipedia.org/wiki/Car) and load it as a LangChain [Document](https://python.langchain.com/v0.2/api_reference/core/documents/langchain_core.documents.base.Document.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84460db2-36e1-4037-bfa6-2a11883c2ba5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:17.778946Z",
     "iopub.status.busy": "2024-09-11T18:18:17.778774Z",
     "iopub.status.idle": "2024-09-11T18:18:18.961597Z",
     "shell.execute_reply": "2024-09-11T18:18:18.961157Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import requests\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "\n",
    "# Download the content\n",
    "response = requests.get(\"https://en.wikipedia.org/wiki/Car\")\n",
    "# Write it to a file\n",
    "with open(\"car.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.text)\n",
    "# Load it with an HTML parser\n",
    "loader = BSHTMLLoader(\"car.html\")\n",
    "document = loader.load()[0]\n",
    "# Clean up code\n",
    "# Replace consecutive new lines with a single new line\n",
    "document.page_content = re.sub(\"\\n\\n+\", \"\\n\", document.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb6917b-123d-4630-a0ce-ed8b293d482d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:18.963743Z",
     "iopub.status.busy": "2024-09-11T18:18:18.963488Z",
     "iopub.status.idle": "2024-09-11T18:18:18.965707Z",
     "shell.execute_reply": "2024-09-11T18:18:18.965444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80427\n"
     ]
    }
   ],
   "source": [
    "print(len(document.page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3ffb8d-587a-4370-886a-e56e617bcb9c",
   "metadata": {},
   "source": [
    "## Define the schema\n",
    "\n",
    "Following the [extraction tutorial](/docs/tutorials/extraction), we will use Pydantic to define the schema of information we wish to extract. In this case, we will extract a list of \"key developments\" (e.g., important historical events) that include a year and description.\n",
    "\n",
    "Note that we also include an `evidence` key and instruct the model to provide in verbatim the relevant sentences of text from the article. This allows us to compare the extraction results to (the model's reconstruction of) text from the original document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3b288ed-87a6-4af0-aac8-20921dc370d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:18.967196Z",
     "iopub.status.busy": "2024-09-11T18:18:18.967103Z",
     "iopub.status.idle": "2024-09-11T18:18:19.043872Z",
     "shell.execute_reply": "2024-09-11T18:18:19.043590Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class KeyDevelopment(BaseModel):\n",
    "    \"\"\"Information about a development in the history of cars.\"\"\"\n",
    "\n",
    "    year: int = Field(\n",
    "        ..., description=\"The year when there was an important historic development.\"\n",
    "    )\n",
    "    description: str = Field(\n",
    "        ..., description=\"What happened in this year? What was the development?\"\n",
    "    )\n",
    "    evidence: str = Field(\n",
    "        ...,\n",
    "        description=\"Repeat in verbatim the sentence(s) from which the year and description information were extracted\",\n",
    "    )\n",
    "\n",
    "\n",
    "class ExtractionData(BaseModel):\n",
    "    \"\"\"Extracted information about key developments in the history of cars.\"\"\"\n",
    "\n",
    "    key_developments: List[KeyDevelopment]\n",
    "\n",
    "\n",
    "# Define a custom prompt to provide instructions and any additional context.\n",
    "# 1) You can add examples into the prompt template to improve extraction quality\n",
    "# 2) Introduce additional parameters to take context into account (e.g., include metadata\n",
    "#    about the document from which the text was extracted.)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert at identifying key historic development in text. \"\n",
    "            \"Only extract important historic developments. Extract nothing if no important information can be found in the text.\",\n",
    "        ),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3909e22e-8a00-4f3d-bbf2-4762a0558af3",
   "metadata": {},
   "source": [
    "## Create an extractor\n",
    "\n",
    "Let's select an LLM. Because we are using tool-calling, we will need a model that supports a tool-calling feature. See [this table](/docs/integrations/chat) for available LLMs.\n",
    "\n",
    "```{=mdx}\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs\n",
    "  customVarName=\"llm\"\n",
    "  openaiParams={`model=\"gpt-4-0125-preview\", temperature=0`}\n",
    "/>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "109f4f05-d0ff-431d-93d9-8f5aa34979a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:19.045580Z",
     "iopub.status.busy": "2024-09-11T18:18:19.045501Z",
     "iopub.status.idle": "2024-09-11T18:18:19.177673Z",
     "shell.execute_reply": "2024-09-11T18:18:19.177373Z"
    }
   },
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4-0125-preview\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa4ae224-6d3d-4fe2-b210-7db19a9fe580",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:19.179295Z",
     "iopub.status.busy": "2024-09-11T18:18:19.179217Z",
     "iopub.status.idle": "2024-09-11T18:18:19.257077Z",
     "shell.execute_reply": "2024-09-11T18:18:19.256824Z"
    }
   },
   "outputs": [],
   "source": [
    "extractor = prompt | llm.with_structured_output(\n",
    "    schema=ExtractionData,\n",
    "    include_raw=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aebafb-26b5-42b2-ae8e-9c05cd56e5c5",
   "metadata": {},
   "source": [
    "## Brute force approach\n",
    "\n",
    "Split the documents into chunks such that each chunk fits into the context window of the LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27b8a373-14b3-45ea-8bf5-9749122ad927",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:19.258430Z",
     "iopub.status.busy": "2024-09-11T18:18:19.258358Z",
     "iopub.status.idle": "2024-09-11T18:18:19.430241Z",
     "shell.execute_reply": "2024-09-11T18:18:19.429870Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    # Controls the size of each chunk\n",
    "    chunk_size=2000,\n",
    "    # Controls overlap between chunks\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_text(document.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b43d7e0-3c85-4d97-86c7-e8c984b60b0a",
   "metadata": {},
   "source": [
    "Use [batch](https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html) functionality to run the extraction in **parallel** across each chunk! \n",
    "\n",
    ":::{.callout-tip}\n",
    "You can often use .batch() to parallelize the extractions! `.batch` uses a threadpool under the hood to help you parallelize workloads.\n",
    "\n",
    "If your model is exposed via an API, this will likely speed up your extraction flow!\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ba766b5-8d6c-48e6-8d69-f391a66b65d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:19.432023Z",
     "iopub.status.busy": "2024-09-11T18:18:19.431913Z",
     "iopub.status.idle": "2024-09-11T18:18:40.316632Z",
     "shell.execute_reply": "2024-09-11T18:18:40.315981Z"
    }
   },
   "outputs": [],
   "source": [
    "# Limit just to the first 3 chunks\n",
    "# so the code can be re-run quickly\n",
    "first_few = texts[:3]\n",
    "\n",
    "extractions = extractor.batch(\n",
    "    [{\"text\": text} for text in first_few],\n",
    "    {\"max_concurrency\": 5},  # limit the concurrency by passing max concurrency!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da8904-e927-406b-a439-2a16f6087ccf",
   "metadata": {},
   "source": [
    "### Merge results\n",
    "\n",
    "After extracting data from across the chunks, we'll want to merge the extractions together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3f77470-ce6c-477f-8957-650913218632",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:40.320586Z",
     "iopub.status.busy": "2024-09-11T18:18:40.320159Z",
     "iopub.status.idle": "2024-09-11T18:18:40.329645Z",
     "shell.execute_reply": "2024-09-11T18:18:40.328866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KeyDevelopment(year=1769, description='Nicolas-Joseph Cugnot built the first full-scale, self-propelled mechanical vehicle, a steam-powered tricycle.', evidence='Nicolas-Joseph Cugnot is widely credited with building the first full-scale, self-propelled mechanical vehicle in about 1769; he created a steam-powered tricycle.'),\n",
       " KeyDevelopment(year=1807, description=\"Nicéphore Niépce and his brother Claude created what was probably the world's first internal combustion engine.\", evidence=\"In 1807, Nicéphore Niépce and his brother Claude created what was probably the world's first internal combustion engine (which they called a Pyréolophore), but installed it in a boat on the river Saone in France.\"),\n",
       " KeyDevelopment(year=1808, description=\"François Isaac de Rivaz designed the world's first vehicle to be powered by an internal combustion engine.\", evidence='Coincidentally, in 1807, the Swiss inventor François Isaac de Rivaz designed his own \"de Rivaz internal combustion engine\", and used it to develop the world\\'s first vehicle to be powered by such an engine.'),\n",
       " KeyDevelopment(year=1881, description='Gustave Trouvé demonstrated a three-wheeled car powered by electricity.', evidence='In November 1881, French inventor Gustave Trouvé demonstrated a three-wheeled car powered by electricity at the International Exposition of Electricity.'),\n",
       " KeyDevelopment(year=1886, description='Carl Benz patented the Benz Patent-Motorwagen, marking the birth of the modern car.', evidence='In 1879, Benz was granted a patent for his first engine, which had been designed in 1878. Many of his other inventions made the use of the internal combustion engine feasible for powering a vehicle. His first Motorwagen was built in 1885 in Mannheim, Germany. He was awarded the patent for its invention as of his application on 29 January 1886 (under the auspices of his major company, Benz & Cie., which was founded in 1883'),\n",
       " KeyDevelopment(year=1886, description='Carl Benz began promotion of his vehicle, marking the beginning of the commercial automobile industry.', evidence='Benz began promotion of the vehicle on 3 July 1886.'),\n",
       " KeyDevelopment(year=1888, description=\"Bertha Benz undertook the first road trip by car to prove the road-worthiness of her husband's invention.\", evidence=\"In August 1888, Bertha Benz, the wife and business partner of Carl Benz, undertook the first road trip by car, to prove the road-worthiness of her husband's invention.\"),\n",
       " KeyDevelopment(year=1896, description='Benz designed and patented the first internal-combustion flat engine, called boxermotor.', evidence='In 1896, Benz designed and patented the first internal-combustion flat engine, called boxermotor.'),\n",
       " KeyDevelopment(year=1897, description='The first motor car in central Europe and one of the first factory-made cars in the world was produced by Nesselsdorfer Wagenbau.', evidence='The first motor car in central Europe and one of the first factory-made cars in the world, was produced by Czech company Nesselsdorfer Wagenbau (later renamed to Tatra) in 1897, the Präsident automobil.'),\n",
       " KeyDevelopment(year=1901, description='Ransom Olds started large-scale, production-line manufacturing of affordable cars.', evidence='Large-scale, production-line manufacturing of affordable cars was started by Ransom Olds in 1901 at his Oldsmobile factory in Lansing, Michigan.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_developments = []\n",
    "\n",
    "for extraction in extractions:\n",
    "    key_developments.extend(extraction.key_developments)\n",
    "\n",
    "key_developments[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48afd4a7-abcd-48b4-8ff1-6ca485f529e3",
   "metadata": {},
   "source": [
    "## RAG based approach\n",
    "\n",
    "Another simple idea is to chunk up the text, but instead of extracting information from every chunk, just focus on the the most relevant chunks.\n",
    "\n",
    ":::{.callout-caution}\n",
    "It can be difficult to identify which chunks are relevant.\n",
    "\n",
    "For example, in the `car` article we're using here, most of the article contains key development information. So by using\n",
    "**RAG**, we'll likely be throwing out a lot of relevant information.\n",
    "\n",
    "We suggest experimenting with your use case and determining whether this approach works or not.\n",
    ":::\n",
    "\n",
    "To implement the RAG based approach: \n",
    "\n",
    "1. Chunk up your document(s) and index them (e.g., in a vectorstore);\n",
    "2. Prepend the `extractor` chain with a retrieval step using the vectorstore.\n",
    "\n",
    "Here's a simple example that relies on the `FAISS` vectorstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaf37c82-625b-4fa1-8e88-73303f08ac16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:40.332298Z",
     "iopub.status.busy": "2024-09-11T18:18:40.332100Z",
     "iopub.status.idle": "2024-09-11T18:18:42.004055Z",
     "shell.execute_reply": "2024-09-11T18:18:42.003566Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "texts = text_splitter.split_text(document.page_content)\n",
    "vectorstore = FAISS.from_texts(texts, embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 1}\n",
    ")  # Only extract from first document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013ecad9-f80f-477c-b954-494b46a02a07",
   "metadata": {},
   "source": [
    "In this case the RAG extractor is only looking at the top document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47aad00b-7013-4f7f-a1b0-02ef269093bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:42.006563Z",
     "iopub.status.busy": "2024-09-11T18:18:42.006403Z",
     "iopub.status.idle": "2024-09-11T18:18:42.009439Z",
     "shell.execute_reply": "2024-09-11T18:18:42.008936Z"
    }
   },
   "outputs": [],
   "source": [
    "rag_extractor = {\n",
    "    \"text\": retriever | (lambda docs: docs[0].page_content)  # fetch content of top doc\n",
    "} | extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68f2de01-0cd8-456e-a959-db236189d41b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:42.011495Z",
     "iopub.status.busy": "2024-09-11T18:18:42.011354Z",
     "iopub.status.idle": "2024-09-11T18:18:43.440017Z",
     "shell.execute_reply": "2024-09-11T18:18:43.439092Z"
    }
   },
   "outputs": [],
   "source": [
    "results = rag_extractor.invoke(\"Key developments associated with cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1788e2d6-77bb-417f-827c-eb96c035164e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:18:43.443888Z",
     "iopub.status.busy": "2024-09-11T18:18:43.443656Z",
     "iopub.status.idle": "2024-09-11T18:18:43.448107Z",
     "shell.execute_reply": "2024-09-11T18:18:43.446792Z"
    }
   },
   "outputs": [],
   "source": [
    "for key_development in results.key_developments:\n",
    "    print(key_development)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf36e626-cf5d-4324-ba29-9bd602be9b97",
   "metadata": {},
   "source": [
    "## Common issues\n",
    "\n",
    "Different methods have their own pros and cons related to cost, speed, and accuracy.\n",
    "\n",
    "Watch out for these issues:\n",
    "\n",
    "* Chunking content means that the LLM can fail to extract information if the information is spread across multiple chunks.\n",
    "* Large chunk overlap may cause the same information to be extracted twice, so be prepared to de-duplicate!\n",
    "* LLMs can make up data. If looking for a single fact across a large text and using a brute force approach, you may end up getting more made up data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
