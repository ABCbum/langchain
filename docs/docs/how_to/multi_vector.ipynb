{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9172545",
   "metadata": {},
   "source": [
    "# How to retrieve using multiple vectors per document\n",
    "\n",
    "It can often be useful to store multiple vectors per document. There are multiple use cases where this is beneficial. For example, we can embed multiple chunks of a document and associate those embeddings with the parent document, allowing retriever hits on the chunks to return the larger document.\n",
    "\n",
    "LangChain implements a base [MultiVectorRetriever](https://python.langchain.com/v0.2/api_reference/langchain/retrievers/langchain.retrievers.multi_vector.MultiVectorRetriever.html), which simplifies this process. Much of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the `MultiVectorRetriever`.\n",
    "\n",
    "The methods to create multiple vectors per document include:\n",
    "\n",
    "- Smaller chunks: split a document into smaller chunks, and embed those (this is [ParentDocumentRetriever](https://python.langchain.com/v0.2/api_reference/langchain/retrievers/langchain.retrievers.parent_document_retriever.ParentDocumentRetriever.html)).\n",
    "- Summary: create a summary for each document, embed that along with (or instead of) the document.\n",
    "- Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document.\n",
    "\n",
    "Note that this also enables another method of adding embeddings - manually. This is useful because you can explicitly add questions or queries that should lead to a document being recovered, giving you more control.\n",
    "\n",
    "Below we walk through an example. First we instantiate some documents. We will index them in an (in-memory) [Chroma](/docs/integrations/providers/chroma/) vector store using [OpenAI](https://python.langchain.com/v0.2/docs/integrations/text_embedding/openai/) embeddings, but any LangChain vector store or embeddings model will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09cecd95-3499-465a-895a-944627ffb77f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:28:35.515658Z",
     "iopub.status.busy": "2024-09-11T18:28:35.515388Z",
     "iopub.status.idle": "2024-09-11T18:28:38.954257Z",
     "shell.execute_reply": "2024-09-11T18:28:38.953740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "langchain-groq 0.2.0.dev1 requires langchain-core<0.4.0,>=0.3.0.dev4, but you have langchain-core 0.2.39 which is incompatible.\r\n",
      "langchain-google-genai 2.0.0.dev1 requires langchain-core<0.4.0,>=0.3.0.dev4, but you have langchain-core 0.2.39 which is incompatible.\r\n",
      "langchain-benchmarks 0.0.14 requires langchain-community<0.3,>=0.2, but you have langchain-community 0.3.0.dev1 which is incompatible.\r\n",
      "langchain-community 0.3.0.dev1 requires langchain<0.4.0,>=0.3.0.dev1, but you have langchain 0.2.16 which is incompatible.\r\n",
      "langchain-community 0.3.0.dev1 requires langchain-core<0.4.0,>=0.3.0.dev2, but you have langchain-core 0.2.39 which is incompatible.\r\n",
      "langchain-anthropic 0.2.0.dev1 requires langchain-core<0.4.0,>=0.3.0.dev4, but you have langchain-core 0.2.39 which is incompatible.\r\n",
      "langchain-mistralai 0.2.0.dev1 requires langchain-core<0.4.0,>=0.3.0.dev4, but you have langchain-core 0.2.39 which is incompatible.\r\n",
      "langchain-google-vertexai 2.0.0.dev1 requires langchain-core<0.4.0,>=0.3.0.dev4, but you have langchain-core 0.2.39 which is incompatible.\r\n",
      "langchain-experimental 0.3.0.dev1 requires langchain-core<0.4.0,>=0.3.0.dev4, but you have langchain-core 0.2.39 which is incompatible.\r\n",
      "langchain-ollama 0.2.0.dev1 requires langchain-core<0.4.0,>=0.3.0.dev4, but you have langchain-core 0.2.39 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain-chroma langchain langchain-openai > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c1421a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:28:38.956667Z",
     "iopub.status.busy": "2024-09-11T18:28:38.956334Z",
     "iopub.status.idle": "2024-09-11T18:28:39.832306Z",
     "shell.execute_reply": "2024-09-11T18:28:39.832070Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loaders = [\n",
    "    TextLoader(\"paul_graham_essay.txt\"),\n",
    "    TextLoader(\"state_of_the_union.txt\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa17beda",
   "metadata": {},
   "source": [
    "## Smaller chunks\n",
    "\n",
    "Often times it can be useful to retrieve larger chunks of information, but embed smaller chunks. This allows for embeddings to capture the semantic meaning as closely as possible, but for as much context as possible to be passed downstream. Note that this is what the [ParentDocumentRetriever](https://python.langchain.com/v0.2/api_reference/langchain/retrievers/langchain.retrievers.parent_document_retriever.ParentDocumentRetriever.html) does. Here we show what is going on under the hood.\n",
    "\n",
    "We will make a distinction between the vector store, which indexes embeddings of the (sub) documents, and the document store, which houses the \"parent\" documents and associates them with an identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e7b6b45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:28:39.833909Z",
     "iopub.status.busy": "2024-09-11T18:28:39.833739Z",
     "iopub.status.idle": "2024-09-11T18:28:39.860044Z",
     "shell.execute_reply": "2024-09-11T18:28:39.859800Z"
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4feded4-856a-4282-91c3-53aabc62e6ff",
   "metadata": {},
   "source": [
    "We next generate the \"sub\" documents by splitting the original documents. Note that we store the document identifier in the `metadata` of the corresponding [Document](https://python.langchain.com/v0.2/api_reference/core/documents/langchain_core.documents.base.Document.html) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d23247d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:28:39.861432Z",
     "iopub.status.busy": "2024-09-11T18:28:39.861358Z",
     "iopub.status.idle": "2024-09-11T18:28:39.870322Z",
     "shell.execute_reply": "2024-09-11T18:28:39.870080Z"
    }
   },
   "outputs": [],
   "source": [
    "# The splitter to use to create smaller chunks\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "sub_docs = []\n",
    "for i, doc in enumerate(docs):\n",
    "    _id = doc_ids[i]\n",
    "    _sub_docs = child_text_splitter.split_documents([doc])\n",
    "    for _doc in _sub_docs:\n",
    "        _doc.metadata[id_key] = _id\n",
    "    sub_docs.extend(_sub_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0634f8-90d5-4250-981a-5257c8a6d455",
   "metadata": {},
   "source": [
    "Finally, we index the documents in our vector store and document store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92ed5861",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:28:39.871746Z",
     "iopub.status.busy": "2024-09-11T18:28:39.871659Z",
     "iopub.status.idle": "2024-09-11T18:28:57.462561Z",
     "shell.execute_reply": "2024-09-11T18:28:57.462196Z"
    }
   },
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(sub_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c48c6d-850c-4317-9b6e-1ade92f2f710",
   "metadata": {},
   "source": [
    "The vector store alone will retrieve small chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8afed60c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:28:57.464483Z",
     "iopub.status.busy": "2024-09-11T18:28:57.464351Z",
     "iopub.status.idle": "2024-09-11T18:28:57.621938Z",
     "shell.execute_reply": "2024-09-11T18:28:57.621391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_id': 'e8e696e2-76ba-4b68-b3e6-7626ac7e8da0', 'source': 'state_of_the_union.txt'}, page_content='Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.vectorstore.similarity_search(\"justice breyer\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717097c7-61d9-4306-8625-ef8f1940c127",
   "metadata": {},
   "source": [
    "Whereas the retriever will return the larger parent document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c9017f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:28:57.625037Z",
     "iopub.status.busy": "2024-09-11T18:28:57.624424Z",
     "iopub.status.idle": "2024-09-11T18:28:58.079771Z",
     "shell.execute_reply": "2024-09-11T18:28:58.079483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9875"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retriever.invoke(\"justice breyer\")[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdef8339-f9fa-4b3b-955f-ad9dbdf2734f",
   "metadata": {},
   "source": [
    "The default search type the retriever performs on the vector database is a similarity search. LangChain vector stores also support searching via [Max Marginal Relevance](https://python.langchain.com/v0.2/api_reference/core/vectorstores/langchain_core.vectorstores.VectorStore.html#langchain_core.vectorstores.VectorStore.max_marginal_relevance_search). This can be controlled via the `search_type` parameter of the retriever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36739460-a737-4a8e-b70f-50bf8c8eaae7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:28:58.081486Z",
     "iopub.status.busy": "2024-09-11T18:28:58.081366Z",
     "iopub.status.idle": "2024-09-11T18:28:58.750199Z",
     "shell.execute_reply": "2024-09-11T18:28:58.749152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9875"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers.multi_vector import SearchType\n",
    "\n",
    "retriever.search_type = SearchType.mmr\n",
    "\n",
    "len(retriever.invoke(\"justice breyer\")[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a7ae0d",
   "metadata": {},
   "source": [
    "## Associating summaries with a document for retrieval\n",
    "\n",
    "A summary may be able to distill more accurately what a chunk is about, leading to better retrieval. Here we show how to create summaries, and then embed those.\n",
    "\n",
    "We construct a simple [chain](/docs/how_to/sequence) that will receive an input [Document](https://python.langchain.com/v0.2/api_reference/core/documents/langchain_core.documents.base.Document.html) object and generate a summary using a LLM.\n",
    "\n",
    "```{=mdx}\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs customVarName=\"llm\" />\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6589291f-55bb-4e9a-b4ff-08f2506ed641",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:28:58.765491Z",
     "iopub.status.busy": "2024-09-11T18:28:58.763364Z",
     "iopub.status.idle": "2024-09-11T18:28:58.798963Z",
     "shell.execute_reply": "2024-09-11T18:28:58.797989Z"
    }
   },
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1433dff4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:28:58.830507Z",
     "iopub.status.busy": "2024-09-11T18:28:58.830019Z",
     "iopub.status.idle": "2024-09-11T18:28:58.837031Z",
     "shell.execute_reply": "2024-09-11T18:28:58.836245Z"
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faa9fde-1b09-4849-a815-8b2e89c30a02",
   "metadata": {},
   "source": [
    "Note that we can [batch](https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable) the chain accross documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41a2a738",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:28:58.840235Z",
     "iopub.status.busy": "2024-09-11T18:28:58.839941Z",
     "iopub.status.idle": "2024-09-11T18:29:06.054864Z",
     "shell.execute_reply": "2024-09-11T18:29:06.054072Z"
    }
   },
   "outputs": [],
   "source": [
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ef599e-140b-4905-8b62-6c52cdde1852",
   "metadata": {},
   "source": [
    "We can then initialize a `MultiVectorRetriever` as before, indexing the summaries in our vector store, and retaining the original documents in our document store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ac5e4b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:29:06.059474Z",
     "iopub.status.busy": "2024-09-11T18:29:06.059144Z",
     "iopub.status.idle": "2024-09-11T18:29:06.934268Z",
     "shell.execute_reply": "2024-09-11T18:29:06.933614Z"
    }
   },
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "862ae920",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:29:06.938108Z",
     "iopub.status.busy": "2024-09-11T18:29:06.937774Z",
     "iopub.status.idle": "2024-09-11T18:29:06.940856Z",
     "shell.execute_reply": "2024-09-11T18:29:06.940194Z"
    }
   },
   "outputs": [],
   "source": [
    "# # We can also add the original chunks to the vectorstore if we so want\n",
    "# for i, doc in enumerate(docs):\n",
    "#     doc.metadata[id_key] = doc_ids[i]\n",
    "# retriever.vectorstore.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0274892-29c1-4616-9040-d23f9d537526",
   "metadata": {},
   "source": [
    "Querying the vector store will return summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "299232d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:29:06.944113Z",
     "iopub.status.busy": "2024-09-11T18:29:06.943677Z",
     "iopub.status.idle": "2024-09-11T18:29:07.403359Z",
     "shell.execute_reply": "2024-09-11T18:29:07.402760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_id': 'da369112-ae0d-4107-8b01-2cc7a4e9309c'}, page_content=\"President Biden discusses the nomination of Judge Ketanji Brown Jackson to the Supreme Court, the need for immigration reform, protecting women's rights, supporting LGBTQ+ Americans, and advancing unity through bipartisan legislation. He outlines a four-point Unity Agenda focused on addressing the opioid epidemic, mental health, supporting veterans, and ending cancer. The President expresses optimism about America's future and emphasizes the nation's strength lies in its people working together. He concludes with a message of unity and hope for the future of the United States.\")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_docs = retriever.vectorstore.similarity_search(\"justice breyer\")\n",
    "\n",
    "sub_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f77ac5-2926-4f60-aad5-b2067900dff9",
   "metadata": {},
   "source": [
    "Whereas the retriever will return the larger source document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4cce5c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:29:07.406590Z",
     "iopub.status.busy": "2024-09-11T18:29:07.406318Z",
     "iopub.status.idle": "2024-09-11T18:29:07.754971Z",
     "shell.execute_reply": "2024-09-11T18:29:07.751078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9194"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"justice breyer\")\n",
    "\n",
    "len(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097a5396",
   "metadata": {},
   "source": [
    "## Hypothetical Queries\n",
    "\n",
    "An LLM can also be used to generate a list of hypothetical questions that could be asked of a particular document, which might bear close semantic similarity to relevant queries in a [RAG](/docs/tutorials/rag) application. These questions can then be embedded and associated with the documents to improve retrieval.\n",
    "\n",
    "Below, we use the [with_structured_output](/docs/how_to/structured_output/) method to structure the LLM output into a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03d85234-c33a-4a43-861d-47328e1ec2ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:29:07.758627Z",
     "iopub.status.busy": "2024-09-11T18:29:07.758350Z",
     "iopub.status.idle": "2024-09-11T18:29:07.792503Z",
     "shell.execute_reply": "2024-09-11T18:29:07.792035Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class HypotheticalQuestions(BaseModel):\n",
    "    \"\"\"Generate hypothetical questions.\"\"\"\n",
    "\n",
    "    questions: List[str] = Field(..., description=\"List of questions\")\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    # Only asking for 3 hypothetical questions, but this could be adjusted\n",
    "    | ChatPromptTemplate.from_template(\n",
    "        \"Generate a list of exactly 3 hypothetical questions that the below document could be used to answer:\\n\\n{doc}\"\n",
    "    )\n",
    "    | ChatOpenAI(max_retries=0, model=\"gpt-4o\").with_structured_output(\n",
    "        HypotheticalQuestions\n",
    "    )\n",
    "    | (lambda x: x.questions)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dddc40f-62af-413c-b944-f94a5e1f2f4e",
   "metadata": {},
   "source": [
    "Invoking the chain on a single document demonstrates that it outputs a list of questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11d30554",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:29:07.794921Z",
     "iopub.status.busy": "2024-09-11T18:29:07.794763Z",
     "iopub.status.idle": "2024-09-11T18:29:09.520596Z",
     "shell.execute_reply": "2024-09-11T18:29:09.520067Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What were the author's early experiences with programming and writing before college?\",\n",
       " \"How did the author's perspective on philosophy change during college?\",\n",
       " 'What led the author to switch their focus from AI to Lisp in grad school?']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcffc572-7b20-4b77-857a-90ec360a8f7e",
   "metadata": {},
   "source": [
    "We can batch then batch the chain over all documents and assemble our vector store and document store as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2cd6e75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:29:09.525803Z",
     "iopub.status.busy": "2024-09-11T18:29:09.525379Z",
     "iopub.status.idle": "2024-09-11T18:29:15.873603Z",
     "shell.execute_reply": "2024-09-11T18:29:15.873214Z"
    }
   },
   "outputs": [],
   "source": [
    "# Batch chain over documents to generate hypothetical questions\n",
    "hypothetical_questions = chain.batch(docs, {\"max_concurrency\": 5})\n",
    "\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"hypo-questions\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "\n",
    "# Generate Document objects from hypothetical questions\n",
    "question_docs = []\n",
    "for i, question_list in enumerate(hypothetical_questions):\n",
    "    question_docs.extend(\n",
    "        [Document(page_content=s, metadata={id_key: doc_ids[i]}) for s in question_list]\n",
    "    )\n",
    "\n",
    "\n",
    "retriever.vectorstore.add_documents(question_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cba8ab-a06f-4545-85fc-cf49d0204b5e",
   "metadata": {},
   "source": [
    "Note that querying the underlying vector store will retrieve hypothetical questions that are semantically similar to the input query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b442b90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:29:15.876089Z",
     "iopub.status.busy": "2024-09-11T18:29:15.875913Z",
     "iopub.status.idle": "2024-09-11T18:29:16.806780Z",
     "shell.execute_reply": "2024-09-11T18:29:16.806237Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'doc_id': '2fc562eb-3edc-424a-b935-f08b95591ca6'}, page_content='What are the qualifications and background of Judge Ketanji Brown Jackson, the nominee for the United States Supreme Court?'),\n",
       " Document(metadata={'doc_id': '2fc562eb-3edc-424a-b935-f08b95591ca6'}, page_content='What measures are being taken to secure the U.S. border and reform the immigration system?'),\n",
       " Document(metadata={'doc_id': '8908a9ef-409b-496c-9d63-800707b3d5be'}, page_content='What might have happened if Robert Morris had never given the unsolicited advice to quit Y Combinator?'),\n",
       " Document(metadata={'doc_id': 'd8f62569-cfa1-4188-9c8a-1da3da1f6375'}, page_content=\"What impact does the Bipartisan Infrastructure Law have on improving America's infrastructure and economy?\")]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_docs = retriever.vectorstore.similarity_search(\"justice breyer\")\n",
    "\n",
    "sub_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c32e43-5f4a-463b-a0c2-2101986f70e6",
   "metadata": {},
   "source": [
    "And invoking the retriever will return the corresponding document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7594b24e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:29:16.810354Z",
     "iopub.status.busy": "2024-09-11T18:29:16.809905Z",
     "iopub.status.idle": "2024-09-11T18:29:17.458997Z",
     "shell.execute_reply": "2024-09-11T18:29:17.458339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9194"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"justice breyer\")\n",
    "len(retrieved_docs[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
