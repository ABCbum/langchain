{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5715368",
   "metadata": {},
   "source": [
    "# How to track token usage in ChatModels\n",
    "\n",
    ":::info Prerequisites\n",
    "\n",
    "This guide assumes familiarity with the following concepts:\n",
    "- [Chat models](/docs/concepts/#chat-models)\n",
    "\n",
    ":::\n",
    "\n",
    "Tracking token usage to calculate cost is an important part of putting your app in production. This guide goes over how to obtain this information from your LangChain model calls.\n",
    "\n",
    "This guide requires `langchain-openai >= 0.1.9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c7d1338-dd1b-4d06-b33d-d5cffc49fd6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:09:23.504824Z",
     "iopub.status.busy": "2024-09-11T18:09:23.504526Z",
     "iopub.status.idle": "2024-09-11T18:09:24.795590Z",
     "shell.execute_reply": "2024-09-11T18:09:24.794983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet langchain langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ae1e2-a52d-4459-81fd-cdc68b06742a",
   "metadata": {},
   "source": [
    "## Using LangSmith\n",
    "\n",
    "You can use [LangSmith](https://www.langchain.com/langsmith) to help track token usage in your LLM application. See the [LangSmith quick start guide](https://docs.smith.langchain.com/).\n",
    "\n",
    "## Using AIMessage.usage_metadata\n",
    "\n",
    "A number of model providers return token usage information as part of the chat generation response. When available, this information will be included on the `AIMessage` objects produced by the corresponding model.\n",
    "\n",
    "LangChain `AIMessage` objects include a [usage_metadata](https://python.langchain.com/v0.2/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html#langchain_core.messages.ai.AIMessage.usage_metadata) attribute. When populated, this attribute will be a [UsageMetadata](https://python.langchain.com/v0.2/api_reference/core/messages/langchain_core.messages.ai.UsageMetadata.html) dictionary with standard keys (e.g., `\"input_tokens\"` and `\"output_tokens\"`).\n",
    "\n",
    "Examples:\n",
    "\n",
    "**OpenAI**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b39bf807-4125-4db4-bbf7-28a46afff6b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:09:24.798095Z",
     "iopub.status.busy": "2024-09-11T18:09:24.797900Z",
     "iopub.status.idle": "2024-09-11T18:09:26.012329Z",
     "shell.execute_reply": "2024-09-11T18:09:26.011876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # !pip install -qU langchain-openai\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "openai_response = llm.invoke(\"hello\")\n",
    "openai_response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2299c44a-2fe6-4d52-a6a2-99ff6d231c73",
   "metadata": {},
   "source": [
    "**Anthropic**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c82ff80-ec4e-4049-b019-5f0bbd7df82a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:09:26.014584Z",
     "iopub.status.busy": "2024-09-11T18:09:26.014418Z",
     "iopub.status.idle": "2024-09-11T18:09:26.741176Z",
     "shell.execute_reply": "2024-09-11T18:09:26.740580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 8, 'output_tokens': 12, 'total_tokens': 20}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install -qU langchain-anthropic\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "anthropic_response = llm.invoke(\"hello\")\n",
    "anthropic_response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4efc15-ba9f-4b3d-9278-8e01f99f263f",
   "metadata": {},
   "source": [
    "### Using AIMessage.response_metadata\n",
    "\n",
    "Metadata from the model response is also included in the AIMessage [response_metadata](https://python.langchain.com/v0.2/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html#langchain_core.messages.ai.AIMessage.response_metadata) attribute. These data are typically not standardized. Note that different providers adopt different conventions for representing token counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f156f9da-21f2-4c81-a714-54cbf9ad393e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:09:26.744883Z",
     "iopub.status.busy": "2024-09-11T18:09:26.744439Z",
     "iopub.status.idle": "2024-09-11T18:09:26.748008Z",
     "shell.execute_reply": "2024-09-11T18:09:26.747501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI: {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17}\n",
      "\n",
      "Anthropic: {'input_tokens': 8, 'output_tokens': 12}\n"
     ]
    }
   ],
   "source": [
    "print(f'OpenAI: {openai_response.response_metadata[\"token_usage\"]}\\n')\n",
    "print(f'Anthropic: {anthropic_response.response_metadata[\"usage\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ef2c43-0ff6-49eb-9782-e4070c9da8d7",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "Some providers support token count metadata in a streaming context.\n",
    "\n",
    "#### OpenAI\n",
    "\n",
    "For example, OpenAI will return a message [chunk](https://python.langchain.com/v0.2/api_reference/core/messages/langchain_core.messages.ai.AIMessageChunk.html) at the end of a stream with token usage information. This behavior is supported by `langchain-openai >= 0.1.9` and can be enabled by setting `stream_usage=True`. This attribute can also be set when `ChatOpenAI` is instantiated.\n",
    "\n",
    "```{=mdx}\n",
    ":::note\n",
    "By default, the last message chunk in a stream will include a `\"finish_reason\"` in the message's `response_metadata` attribute. If we include token usage in streaming mode, an additional chunk containing usage metadata will be added to the end of the stream, such that `\"finish_reason\"` appears on the second to last message chunk.\n",
    ":::\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07f0c872-6b6c-4fed-a129-9b5a858505be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:09:26.750492Z",
     "iopub.status.busy": "2024-09-11T18:09:26.750290Z",
     "iopub.status.idle": "2024-09-11T18:09:28.773040Z",
     "shell.execute_reply": "2024-09-11T18:09:28.772546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={} response_metadata={} id='run-c8896c9b-d62b-4e02-bfda-557eefd52d5a'\n",
      "content='Hello' additional_kwargs={} response_metadata={} id='run-c8896c9b-d62b-4e02-bfda-557eefd52d5a'\n",
      "content='!' additional_kwargs={} response_metadata={} id='run-c8896c9b-d62b-4e02-bfda-557eefd52d5a'\n",
      "content=' How' additional_kwargs={} response_metadata={} id='run-c8896c9b-d62b-4e02-bfda-557eefd52d5a'\n",
      "content=' can' additional_kwargs={} response_metadata={} id='run-c8896c9b-d62b-4e02-bfda-557eefd52d5a'\n",
      "content=' I' additional_kwargs={} response_metadata={} id='run-c8896c9b-d62b-4e02-bfda-557eefd52d5a'\n",
      "content=' assist' additional_kwargs={} response_metadata={} id='run-c8896c9b-d62b-4e02-bfda-557eefd52d5a'\n",
      "content=' you' additional_kwargs={} response_metadata={} id='run-c8896c9b-d62b-4e02-bfda-557eefd52d5a'\n",
      "content=' today' additional_kwargs={} response_metadata={} id='run-c8896c9b-d62b-4e02-bfda-557eefd52d5a'\n",
      "content='?' additional_kwargs={} response_metadata={} id='run-c8896c9b-d62b-4e02-bfda-557eefd52d5a'\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857'} id='run-c8896c9b-d62b-4e02-bfda-557eefd52d5a'\n",
      "content='' additional_kwargs={} response_metadata={} id='run-c8896c9b-d62b-4e02-bfda-557eefd52d5a' usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17}\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "aggregate = None\n",
    "for chunk in llm.stream(\"hello\", stream_usage=True):\n",
    "    print(chunk)\n",
    "    aggregate = chunk if aggregate is None else aggregate + chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd809ded-8b13-4d5f-be5e-277b79d51802",
   "metadata": {},
   "source": [
    "Note that the usage metadata will be included in the sum of the individual message chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3db7bc03-a7d4-4704-92ab-f8ba92ef59ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:09:28.776148Z",
     "iopub.status.busy": "2024-09-11T18:09:28.775905Z",
     "iopub.status.idle": "2024-09-11T18:09:28.779435Z",
     "shell.execute_reply": "2024-09-11T18:09:28.778815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n",
      "{'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17}\n"
     ]
    }
   ],
   "source": [
    "print(aggregate.content)\n",
    "print(aggregate.usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dba63e8-0ed7-4533-8f0f-78e19c38a25c",
   "metadata": {},
   "source": [
    "To disable streaming token counts for OpenAI, set `stream_usage` to False, or omit it from the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67117f2b-ce68-4c1e-9556-2d3849f90e1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:09:28.783054Z",
     "iopub.status.busy": "2024-09-11T18:09:28.782814Z",
     "iopub.status.idle": "2024-09-11T18:09:29.268093Z",
     "shell.execute_reply": "2024-09-11T18:09:29.267777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={} response_metadata={} id='run-1d3bd183-2557-40c6-b24d-c4b4caf9749e'\n",
      "content='Hello' additional_kwargs={} response_metadata={} id='run-1d3bd183-2557-40c6-b24d-c4b4caf9749e'\n",
      "content='!' additional_kwargs={} response_metadata={} id='run-1d3bd183-2557-40c6-b24d-c4b4caf9749e'\n",
      "content=' How' additional_kwargs={} response_metadata={} id='run-1d3bd183-2557-40c6-b24d-c4b4caf9749e'\n",
      "content=' can' additional_kwargs={} response_metadata={} id='run-1d3bd183-2557-40c6-b24d-c4b4caf9749e'\n",
      "content=' I' additional_kwargs={} response_metadata={} id='run-1d3bd183-2557-40c6-b24d-c4b4caf9749e'\n",
      "content=' assist' additional_kwargs={} response_metadata={} id='run-1d3bd183-2557-40c6-b24d-c4b4caf9749e'\n",
      "content=' you' additional_kwargs={} response_metadata={} id='run-1d3bd183-2557-40c6-b24d-c4b4caf9749e'\n",
      "content=' today' additional_kwargs={} response_metadata={} id='run-1d3bd183-2557-40c6-b24d-c4b4caf9749e'\n",
      "content='?' additional_kwargs={} response_metadata={} id='run-1d3bd183-2557-40c6-b24d-c4b4caf9749e'\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857'} id='run-1d3bd183-2557-40c6-b24d-c4b4caf9749e'\n"
     ]
    }
   ],
   "source": [
    "aggregate = None\n",
    "for chunk in llm.stream(\"hello\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5d9617-be3a-419a-9276-de9c29fa50ae",
   "metadata": {},
   "source": [
    "You can also enable streaming token usage by setting `stream_usage` when instantiating the chat model. This can be useful when incorporating chat models into LangChain [chains](/docs/concepts#langchain-expression-language-lcel): usage metadata can be monitored when [streaming intermediate steps](/docs/how_to/streaming#using-stream-events) or using tracing software such as [LangSmith](https://docs.smith.langchain.com/).\n",
    "\n",
    "See the below example, where we return output structured to a desired schema, but can still observe token usage streamed from intermediate steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b1523d8-127e-4314-82fa-bd97aca37f9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:09:29.269825Z",
     "iopub.status.busy": "2024-09-11T18:09:29.269703Z",
     "iopub.status.idle": "2024-09-11T18:09:29.877644Z",
     "shell.execute_reply": "2024-09-11T18:09:29.877227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token usage: {'input_tokens': 74, 'output_tokens': 21, 'total_tokens': 95}\n",
      "\n",
      "setup=\"Why don't scientists trust atoms?\" punchline='Because they make up everything!'\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    stream_usage=True,\n",
    ")\n",
    "# Under the hood, .with_structured_output binds tools to the\n",
    "# chat model and appends a parser.\n",
    "structured_llm = llm.with_structured_output(Joke)\n",
    "\n",
    "async for event in structured_llm.astream_events(\"Tell me a joke\", version=\"v2\"):\n",
    "    if event[\"event\"] == \"on_chat_model_end\":\n",
    "        print(f'Token usage: {event[\"data\"][\"output\"].usage_metadata}\\n')\n",
    "    elif event[\"event\"] == \"on_chain_end\":\n",
    "        print(event[\"data\"][\"output\"])\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc8d313-4bef-463e-89a5-236d8bb6ab2f",
   "metadata": {},
   "source": [
    "Token usage is also visible in the corresponding [LangSmith trace](https://smith.langchain.com/public/fe6513d5-7212-4045-82e0-fefa28bc7656/r) in the payload from the chat model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6845407-af25-4eed-bc3e-50925c6661e0",
   "metadata": {},
   "source": [
    "## Using callbacks\n",
    "\n",
    "There are also some API-specific callback context managers that allow you to track token usage across multiple calls. It is currently only implemented for the OpenAI API and Bedrock Anthropic API.\n",
    "\n",
    "### OpenAI\n",
    "\n",
    "Let's first look at an extremely simple example of tracking token usage for a single Chat model call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b04a4486-72fd-48ce-8f9e-5d281b441195",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:09:29.880350Z",
     "iopub.status.busy": "2024-09-11T18:09:29.880157Z",
     "iopub.status.idle": "2024-09-11T18:09:30.496605Z",
     "shell.execute_reply": "2024-09-11T18:09:30.496088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 29\n",
      "\tPrompt Tokens: 11\n",
      "\tCompletion Tokens: 18\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $1.2449999999999998e-05\n"
     ]
    }
   ],
   "source": [
    "# !pip install -qU langchain-community wikipedia\n",
    "\n",
    "from langchain_community.callbacks.manager import get_openai_callback\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    stream_usage=True,\n",
    ")\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm.invoke(\"Tell me a joke\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab6d27",
   "metadata": {},
   "source": [
    "Anything inside the context manager will get tracked. Here's an example of using it to track multiple calls in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05f22a1d-b021-490f-8840-f628a07459f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:09:30.499787Z",
     "iopub.status.busy": "2024-09-11T18:09:30.499531Z",
     "iopub.status.idle": "2024-09-11T18:09:31.704741Z",
     "shell.execute_reply": "2024-09-11T18:09:31.703517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result = llm.invoke(\"Tell me a joke\")\n",
    "    result2 = llm.invoke(\"Tell me a joke\")\n",
    "    print(cb.total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c00c9158-7bb4-4279-88e6-ea70f46e6ac2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:09:31.707797Z",
     "iopub.status.busy": "2024-09-11T18:09:31.707422Z",
     "iopub.status.idle": "2024-09-11T18:09:32.628372Z",
     "shell.execute_reply": "2024-09-11T18:09:32.627777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 29\n",
      "\tPrompt Tokens: 11\n",
      "\tCompletion Tokens: 18\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $1.2449999999999998e-05\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    for chunk in llm.stream(\"Tell me a joke\"):\n",
    "        pass\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8186e7b",
   "metadata": {},
   "source": [
    "If a chain or agent with multiple steps in it is used, it will track all those steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d1125c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:09:32.632102Z",
     "iopub.status.busy": "2024-09-11T18:09:32.631662Z",
     "iopub.status.idle": "2024-09-11T18:09:33.141151Z",
     "shell.execute_reply": "2024-09-11T18:09:33.140877Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent, load_tools\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You're a helpful assistant\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")\n",
    "tools = load_tools([\"wikipedia\"])\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3950d88b-8bfb-4294-b75b-e6fd421e633c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:09:33.143208Z",
     "iopub.status.busy": "2024-09-11T18:09:33.143094Z",
     "iopub.status.idle": "2024-09-11T18:09:41.013864Z",
     "shell.execute_reply": "2024-09-11T18:09:41.012959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Hummingbird'}`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mPage: Hummingbird\n",
      "Summary: Hummingbirds are birds native to the Americas and comprise the biological family Trochilidae. With approximately 366 species and 113 genera, they occur from Alaska to Tierra del Fuego, but most species are found in Central and South America. As of 2024, 21 hummingbird species are listed as endangered or critically endangered, with numerous species declining in population.\n",
      "Hummingbirds have varied specialized characteristics to enable rapid, maneuverable flight: exceptional metabolic capacity, adaptations to high altitude, sensitive visual and communication abilities, and long-distance migration in some species. Among all birds, male hummingbirds have the widest diversity of plumage color, particularly in blues, greens, and purples. Hummingbirds are the smallest mature birds, measuring 7.5–13 cm (3–5 in) in length. The smallest is the 5 cm (2.0 in) bee hummingbird, which weighs less than 2.0 g (0.07 oz), and the largest is the 23 cm (9 in) giant hummingbird, weighing 18–24 grams (0.63–0.85 oz). Noted for long beaks, hummingbirds are specialized for feeding on flower nectar, but all species also consume small insects.\n",
      "They are known as hummingbirds because of the humming sound created by their beating wings, which flap at high frequencies audible to other birds and humans. They hover at rapid wing-flapping rates, which vary from around 12 beats per second in the largest species to 80 per second in small hummingbirds.\n",
      "Hummingbirds have the highest mass-specific metabolic rate of any homeothermic animal. To conserve energy when food is scarce and at night when not foraging, they can enter torpor, a state similar to hibernation, and slow their metabolic rate to 1⁄15 of its normal rate. While most hummingbirds do not migrate, the rufous hummingbird has one of the longest migrations among birds, traveling twice per year between Alaska and Mexico, a distance of about 3,900 miles (6,300 km).\n",
      "Hummingbirds split from their sister group, the swifts and treeswifts, around 42 million years ago. The oldest known fossil hummingbird is Eurotrochilus, from the Rupelian Stage of Early Oligocene Europe.\n",
      "\n",
      "Page: Anna's hummingbird\n",
      "Summary: Anna's hummingbird (Calypte anna) is a North American species of hummingbird. It was named after Anna Masséna, Duchess of Rivoli.\n",
      "It is native to western coastal regions of North America. In the early 20th century, Anna's hummingbirds bred only in northern Baja California and Southern California. The transplanting of exotic ornamental plants in residential areas throughout the Pacific coast and inland deserts provided expanded nectar and nesting sites, allowing the species to expand its breeding range. Year-round residence of Anna's hummingbirds in the Pacific Northwest is an example of ecological release dependent on acclimation to colder winter temperatures, introduced plants, and human provision of nectar feeders during winter.\n",
      "These birds feed on nectar from flowers using a long extendable tongue. They also consume small insects and other arthropods caught in flight or gleaned from vegetation.\n",
      "\n",
      "Page: Bee hummingbird\n",
      "Summary: The bee hummingbird, zunzuncito or Helena hummingbird (Mellisuga helenae) is a species of hummingbird, native to the island of Cuba in the Caribbean. It is the smallest known bird. The bee hummingbird feeds on nectar of flowers and bugs found in Cuba.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'fastest bird species'}`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mPage: List of birds by flight speed\n",
      "Summary: This is a list of the fastest flying birds in the world. A bird's velocity is necessarily variable; a hunting bird will reach much greater speeds while diving to catch prey than when flying horizontally. The bird that can achieve the greatest airspeed is the peregrine falcon (Falco peregrinus), able to exceed 320 km/h (200 mph) in its dives. A close relative of the common swift, the white-throated needletail (Hirundapus caudacutus), is commonly reported as the fastest bird in level flight with a reported top speed of 169 km/h (105 mph). This record remains unconfirmed as the measurement methods have never been published or verified. The record for the fastest confirmed level flight by a bird is 111.5 km/h (69.3 mph) held by the common swift.\n",
      "\n",
      "Page: Fastest animals\n",
      "Summary: This is a list of the fastest animals in the world, by types of animal.\n",
      "\n",
      "Page: Falcon\n",
      "Summary: Falcons () are birds of prey in the genus Falco, which includes about 40 species. Some small species of falcons with long, narrow wings are called hobbies, and some that hover while hunting are called kestrels. Falcons are widely distributed on all continents of the world except Antarctica, though closely related raptors did occur there in the Eocene.\n",
      "Adult falcons have thin, tapered wings, which enable them to fly at high speed and change direction rapidly. Fledgling falcons, in their first year of flying, have longer flight feathers, which make their configuration more like that of a general-purpose bird such as a broadwing. This makes flying easier while still learning the aerial skills required to be effective hunters like the adults.\n",
      "The falcons are the largest genus in the Falconinae subfamily of Falconidae, which also includes two other subfamilies comprising caracaras and a few other species of \"falcons\". All these birds kill prey with their beaks, using a tomial \"tooth\" on the side of their beaks — unlike the hawks, eagles and other larger birds of prey from the unrelated family Accipitridae, who use talons on their feet.\n",
      "The largest falcon is the gyrfalcon at up to 65 cm (26 in) in length.  The smallest falcon species is the pygmy falcon, which measures just 20 cm (7.9 in).  As with hawks and owls, falcons exhibit sexual dimorphism, with the females typically larger than the males, thus allowing a wider range of prey species.\n",
      "As is the case with many birds of prey, falcons have exceptional powers of vision; the visual acuity of one species has been measured at 2.6 times that of human eyes. They are incredibly fast fliers, with the Peregrine falcons having been recorded diving at speeds of 320 km/h (200 mph), making them the fastest-moving creatures on Earth; the fastest recorded dive attained a vertical speed of 390 km/h (240 mph).\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThe scientific name for hummingbirds is **Trochilidae**, which is the biological family that encompasses approximately 366 species of these birds.\n",
      "\n",
      "The fastest bird species is the **peregrine falcon** (*Falco peregrinus*), which can exceed speeds of 320 km/h (200 mph) during its hunting dives. In terms of level flight, the common swift (*Apus apus*) holds the record for the fastest confirmed speed at 111.5 km/h (69.3 mph).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Total Tokens: 1797\n",
      "Prompt Tokens: 1648\n",
      "Completion Tokens: 149\n",
      "Total Cost (USD): $0.00033659999999999994\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    response = agent_executor.invoke(\n",
    "        {\n",
    "            \"input\": \"What's a hummingbird's scientific name and what's the fastest bird species?\"\n",
    "        }\n",
    "    )\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc9122b-050b-4006-b763-264b0b26d9df",
   "metadata": {},
   "source": [
    "### Bedrock Anthropic\n",
    "\n",
    "The `get_bedrock_anthropic_callback` works very similarly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1837c807-136a-49d8-9c33-060e58dc16d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:09:41.018776Z",
     "iopub.status.busy": "2024-09-11T18:09:41.017910Z",
     "iopub.status.idle": "2024-09-11T18:09:41.398058Z",
     "shell.execute_reply": "2024-09-11T18:09:41.397733Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bagatur/langchain-aws/libs/aws/langchain_aws/chat_models/__init__.py:1: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_aws.chat_models.bedrock import BedrockChat, ChatBedrock\n",
      "/Users/bagatur/langchain/.venv/lib/python3.11/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "ename": "SchemaError",
     "evalue": "Invalid Schema:\nmodel.config.extra_fields_behavior\n  Input should be 'allow', 'forbid' or 'ignore' [type=literal_error, input_value=<Extra.forbid: 'forbid'>, input_type=Extra]\n    For further information visit https://errors.pydantic.dev/2.9/v/literal_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSchemaError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# !pip install langchain-aws\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_aws\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatBedrock\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_bedrock_anthropic_callback\n\u001b[1;32m      5\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatBedrock(model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic.claude-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/langchain-aws/libs/aws/langchain_aws/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_aws\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BedrockChat, ChatBedrock, ChatBedrockConverse\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_aws\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BedrockEmbeddings\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_aws\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraphs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NeptuneAnalyticsGraph, NeptuneGraph\n",
      "File \u001b[0;32m~/langchain-aws/libs/aws/langchain_aws/chat_models/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_aws\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbedrock\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BedrockChat, ChatBedrock\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_aws\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbedrock_converse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatBedrockConverse\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBedrockChat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatBedrock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatBedrockConverse\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/langchain-aws/libs/aws/langchain_aws/chat_models/bedrock.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseTool\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypeBaseModel, is_basemodel_subclass\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_aws\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbedrock_converse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatBedrockConverse\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_aws\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction_calling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     40\u001b[0m     ToolsOutputParser,\n\u001b[1;32m     41\u001b[0m     _lc_tool_calls_to_anthropic_tool_use_blocks,\n\u001b[1;32m     42\u001b[0m     convert_to_anthropic_tool,\n\u001b[1;32m     43\u001b[0m     get_system_message,\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_aws\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbedrock\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     46\u001b[0m     BedrockBase,\n\u001b[1;32m     47\u001b[0m     _combine_generation_info_for_llm_result,\n\u001b[1;32m     48\u001b[0m )\n",
      "File \u001b[0;32m~/langchain-aws/libs/aws/langchain_aws/chat_models/bedrock_converse.py:49\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypeBaseModel, is_basemodel_subclass\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_aws\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction_calling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToolsOutputParser\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mChatBedrockConverse\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseChatModel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"Bedrock chat model integration built on the Bedrock converse API.\u001b[39;49;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;43;03m    This implementation will eventually replace the existing ChatBedrock implementation\u001b[39;49;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;43;03m             'metrics': {'latencyMs': 1290}}\u001b[39;49;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mAny\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#: :meta private:\u001b[39;49;00m\n",
      "File \u001b[0;32m~/langchain/.venv/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py:224\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[0;34m(mcs, cls_name, bases, namespace, __pydantic_generic_metadata__, __pydantic_reset_parent_namespace__, _create_model_module, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_wrapper\u001b[38;5;241m.\u001b[39mfrozen \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__hash__\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m namespace:\n\u001b[1;32m    222\u001b[0m     set_default_hash_func(\u001b[38;5;28mcls\u001b[39m, bases)\n\u001b[0;32m--> 224\u001b[0m \u001b[43mcomplete_model_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcls_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraise_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypes_namespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypes_namespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_model_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_create_model_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# If this is placed before the complete_model_class call above,\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# the generic computed fields return type is set to PydanticUndefined\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_computed_fields \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39minfo \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_decorators__\u001b[38;5;241m.\u001b[39mcomputed_fields\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/langchain/.venv/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py:583\u001b[0m, in \u001b[0;36mcomplete_model_class\u001b[0;34m(cls, cls_name, config_wrapper, raise_errors, types_namespace, create_model_module)\u001b[0m\n\u001b[1;32m    580\u001b[0m core_config \u001b[38;5;241m=\u001b[39m config_wrapper\u001b[38;5;241m.\u001b[39mcore_config(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 583\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[43mgen_schema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclean_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m gen_schema\u001b[38;5;241m.\u001b[39mCollectedInvalid:\n\u001b[1;32m    585\u001b[0m     set_model_mocks(\u001b[38;5;28mcls\u001b[39m, cls_name)\n",
      "File \u001b[0;32m~/langchain/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:595\u001b[0m, in \u001b[0;36mGenerateSchema.clean_schema\u001b[0;34m(self, schema)\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCollectedInvalid()\n\u001b[1;32m    594\u001b[0m schema \u001b[38;5;241m=\u001b[39m _discriminated_union\u001b[38;5;241m.\u001b[39mapply_discriminators(schema)\n\u001b[0;32m--> 595\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_core_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m schema\n",
      "File \u001b[0;32m~/langchain/.venv/lib/python3.11/site-packages/pydantic/_internal/_core_utils.py:570\u001b[0m, in \u001b[0;36mvalidate_core_schema\u001b[0;34m(schema)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPYDANTIC_SKIP_VALIDATING_CORE_SCHEMAS\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m schema\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _validate_core_schema(schema)\n",
      "\u001b[0;31mSchemaError\u001b[0m: Invalid Schema:\nmodel.config.extra_fields_behavior\n  Input should be 'allow', 'forbid' or 'ignore' [type=literal_error, input_value=<Extra.forbid: 'forbid'>, input_type=Extra]\n    For further information visit https://errors.pydantic.dev/2.9/v/literal_error"
     ]
    }
   ],
   "source": [
    "# !pip install langchain-aws\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_community.callbacks.manager import get_bedrock_anthropic_callback\n",
    "\n",
    "llm = ChatBedrock(model_id=\"anthropic.claude-v2\")\n",
    "\n",
    "with get_bedrock_anthropic_callback() as cb:\n",
    "    result = llm.invoke(\"Tell me a joke\")\n",
    "    result2 = llm.invoke(\"Tell me a joke\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33172f31",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "You've now seen a few examples of how to track token usage for supported providers.\n",
    "\n",
    "Next, check out the other how-to guides chat models in this section, like [how to get a model to return structured output](/docs/how_to/structured_output) or [how to add caching to your chat models](/docs/how_to/chat_model_caching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb40375d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
