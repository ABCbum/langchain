{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b843b5c4",
   "metadata": {},
   "source": [
    "# How to cache LLM responses\n",
    "\n",
    "LangChain provides an optional caching layer for LLMs. This is useful for two reasons:\n",
    "\n",
    "It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times.\n",
    "It can speed up your application by reducing the number of API calls you make to the LLM provider.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b0b0fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:20:44.604397Z",
     "iopub.status.busy": "2024-09-11T18:20:44.604080Z",
     "iopub.status.idle": "2024-09-11T18:20:45.993146Z",
     "shell.execute_reply": "2024-09-11T18:20:45.992833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "StdinNotImplementedError",
     "evalue": "getpass was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgetpass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getpass\n\u001b[0;32m----> 6\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mgetpass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Please manually enter OpenAI Key\u001b[39;00m\n",
      "File \u001b[0;32m~/langchain/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py:1256\u001b[0m, in \u001b[0;36mKernel.getpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1255\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetpass was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1258\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m: getpass was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain_openai langchain_community\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass()\n",
    "# Please manually enter OpenAI Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa6d335",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:20:45.994795Z",
     "iopub.status.busy": "2024-09-11T18:20:45.994652Z",
     "iopub.status.idle": "2024-09-11T18:20:46.545112Z",
     "shell.execute_reply": "2024-09-11T18:20:46.544804Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# To make the caching really obvious, lets use a slower and older model.\n",
    "# Caching supports newer chat models as well.\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", n=2, best_of=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f168ff0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:20:46.546741Z",
     "iopub.status.busy": "2024-09-11T18:20:46.546629Z",
     "iopub.status.idle": "2024-09-11T18:20:47.274570Z",
     "shell.execute_reply": "2024-09-11T18:20:47.274086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.3 ms, sys: 23 ms, total: 46.3 ms\n",
      "Wall time: 722 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy don't scientists trust atoms? \\n\\nBecause they make up everything!\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_core.caches import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce7620fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:20:47.277045Z",
     "iopub.status.busy": "2024-09-11T18:20:47.276754Z",
     "iopub.status.idle": "2024-09-11T18:20:47.280667Z",
     "shell.execute_reply": "2024-09-11T18:20:47.280219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 220 µs, sys: 94 µs, total: 314 µs\n",
      "Wall time: 317 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy don't scientists trust atoms? \\n\\nBecause they make up everything!\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The second time it is, so it goes faster\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab452f4",
   "metadata": {},
   "source": [
    "## SQLite Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e65de83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:20:47.282281Z",
     "iopub.status.busy": "2024-09-11T18:20:47.282153Z",
     "iopub.status.idle": "2024-09-11T18:20:47.415079Z",
     "shell.execute_reply": "2024-09-11T18:20:47.414311Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm .langchain.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0be83715",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:20:47.418359Z",
     "iopub.status.busy": "2024-09-11T18:20:47.418111Z",
     "iopub.status.idle": "2024-09-11T18:20:47.719815Z",
     "shell.execute_reply": "2024-09-11T18:20:47.719075Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can do the same thing with a SQLite cache\n",
    "from langchain_community.cache import SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b427ce7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:20:47.734492Z",
     "iopub.status.busy": "2024-09-11T18:20:47.733888Z",
     "iopub.status.idle": "2024-09-11T18:20:48.270374Z",
     "shell.execute_reply": "2024-09-11T18:20:48.269621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 463 ms, sys: 808 ms, total: 1.27 s\n",
      "Wall time: 531 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nWhy couldn't the bicycle stand up by itself? Because it was two-tired.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87f52611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:20:48.274264Z",
     "iopub.status.busy": "2024-09-11T18:20:48.273897Z",
     "iopub.status.idle": "2024-09-11T18:20:48.355895Z",
     "shell.execute_reply": "2024-09-11T18:20:48.355510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.4 ms, sys: 47.1 ms, total: 76.6 ms\n",
      "Wall time: 76.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nWhy couldn't the bicycle stand up by itself? Because it was two-tired.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The second time it is, so it goes faster\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9bb158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
