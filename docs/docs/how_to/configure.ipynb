{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9ede5870",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 7\n",
    "keywords: [ConfigurableField, configurable_fields, ConfigurableAlternatives, configurable_alternatives, LCEL]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eaf61b",
   "metadata": {},
   "source": [
    "# How to configure runtime chain internals\n",
    "\n",
    ":::info Prerequisites\n",
    "\n",
    "This guide assumes familiarity with the following concepts:\n",
    "- [LangChain Expression Language (LCEL)](/docs/concepts/#langchain-expression-language)\n",
    "- [Chaining runnables](/docs/how_to/sequence/)\n",
    "- [Binding runtime arguments](/docs/how_to/binding/)\n",
    "\n",
    ":::\n",
    "\n",
    "Sometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things within your chains.\n",
    "This can include tweaking parameters such as temperature or even swapping out one model for another.\n",
    "In order to make this experience as easy as possible, we have defined two methods.\n",
    "\n",
    "- A `configurable_fields` method. This lets you configure particular fields of a runnable.\n",
    "  - This is related to the [`.bind`](/docs/how_to/binding) method on runnables, but allows you to specify parameters for a given step in a chain at runtime rather than specifying them beforehand.\n",
    "- A `configurable_alternatives` method. With this method, you can list out alternatives for any particular runnable that can be set during runtime, and swap them for those specified alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2347a11",
   "metadata": {},
   "source": [
    "## Configurable Fields\n",
    "\n",
    "Let's walk through an example that configures chat model fields like temperature at runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40ed76a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:10:31.217564Z",
     "iopub.status.busy": "2024-09-11T18:10:31.216421Z",
     "iopub.status.idle": "2024-09-11T18:10:32.825476Z",
     "shell.execute_reply": "2024-09-11T18:10:32.825183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "StdinNotImplementedError",
     "evalue": "getpass was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgetpass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getpass\n\u001b[0;32m----> 6\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mgetpass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/langchain/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py:1256\u001b[0m, in \u001b[0;36mKernel.getpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1255\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetpass was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1258\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m: getpass was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet langchain langchain-openai\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ba735f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:10:32.827000Z",
     "iopub.status.busy": "2024-09-11T18:10:32.826901Z",
     "iopub.status.idle": "2024-09-11T18:10:34.180177Z",
     "shell.execute_reply": "2024-09-11T18:10:34.179790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='17', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 11, 'total_tokens': 12}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d5e25ba2-4acf-481e-ab4e-fb1cb6115ca7-0', usage_metadata={'input_tokens': 11, 'output_tokens': 1, 'total_tokens': 12})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0).configurable_fields(\n",
    "    temperature=ConfigurableField(\n",
    "        id=\"llm_temperature\",\n",
    "        name=\"LLM Temperature\",\n",
    "        description=\"The temperature of the LLM\",\n",
    "    )\n",
    ")\n",
    "\n",
    "model.invoke(\"pick a random number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f74589",
   "metadata": {},
   "source": [
    "Above, we defined `temperature` as a [`ConfigurableField`](https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.utils.ConfigurableField.html#langchain_core.runnables.utils.ConfigurableField) that we can set at runtime. To do so, we use the [`with_config`](https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.with_config) method like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f83245c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:10:34.186090Z",
     "iopub.status.busy": "2024-09-11T18:10:34.182062Z",
     "iopub.status.idle": "2024-09-11T18:10:34.810727Z",
     "shell.execute_reply": "2024-09-11T18:10:34.810128Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='23', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 11, 'total_tokens': 12}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-aecbcfab-06e6-4dfd-b097-014c0b06649a-0', usage_metadata={'input_tokens': 11, 'output_tokens': 1, 'total_tokens': 12})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.with_config(configurable={\"llm_temperature\": 0.9}).invoke(\"pick a random number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da1fcd2",
   "metadata": {},
   "source": [
    "Note that the passed `llm_temperature` entry in the dict has the same key as the `id` of the `ConfigurableField`.\n",
    "\n",
    "We can also do this to affect just one step that's part of a chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e75ae678",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:10:34.814374Z",
     "iopub.status.busy": "2024-09-11T18:10:34.814117Z",
     "iopub.status.idle": "2024-09-11T18:10:35.553630Z",
     "shell.execute_reply": "2024-09-11T18:10:35.553390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='27', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 14, 'total_tokens': 15}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d7fe6c19-a0cb-407f-b161-ed98d15f1807-0', usage_metadata={'input_tokens': 14, 'output_tokens': 1, 'total_tokens': 15})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"Pick a random number above {x}\")\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"x\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c09fac15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:10:35.555027Z",
     "iopub.status.busy": "2024-09-11T18:10:35.554926Z",
     "iopub.status.idle": "2024-09-11T18:10:35.924524Z",
     "shell.execute_reply": "2024-09-11T18:10:35.923882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='79', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 14, 'total_tokens': 15}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4eeffc7f-d741-471e-be71-f805211e9b01-0', usage_metadata={'input_tokens': 14, 'output_tokens': 1, 'total_tokens': 15})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.with_config(configurable={\"llm_temperature\": 0.9}).invoke({\"x\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9637d0",
   "metadata": {},
   "source": [
    "### With HubRunnables\n",
    "\n",
    "This is useful to allow for switching of prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a9ea077",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:10:35.927864Z",
     "iopub.status.busy": "2024-09-11T18:10:35.927432Z",
     "iopub.status.idle": "2024-09-11T18:10:36.518700Z",
     "shell.execute_reply": "2024-09-11T18:10:36.518410Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bagatur/langchain/.venv/lib/python3.11/site-packages/langsmith/client.py:5519: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  prompt = loads(json.dumps(prompt_object.manifest))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: foo \\nContext: bar \\nAnswer:\", additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.runnables.hub import HubRunnable\n",
    "\n",
    "prompt = HubRunnable(\"rlm/rag-prompt\").configurable_fields(\n",
    "    owner_repo_commit=ConfigurableField(\n",
    "        id=\"hub_commit\",\n",
    "        name=\"Hub Commit\",\n",
    "        description=\"The Hub commit to pull from\",\n",
    "    )\n",
    ")\n",
    "\n",
    "prompt.invoke({\"question\": \"foo\", \"context\": \"bar\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f33f3cf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:10:36.520258Z",
     "iopub.status.busy": "2024-09-11T18:10:36.520150Z",
     "iopub.status.idle": "2024-09-11T18:10:38.630701Z",
     "shell.execute_reply": "2024-09-11T18:10:38.629819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content=\"[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.<</SYS>> \\nQuestion: foo \\nContext: bar \\nAnswer: [/INST]\", additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.with_config(configurable={\"hub_commit\": \"rlm/rag-prompt-llama\"}).invoke(\n",
    "    {\"question\": \"foo\", \"context\": \"bar\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d51519",
   "metadata": {},
   "source": [
    "## Configurable Alternatives\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac733d35",
   "metadata": {},
   "source": [
    "The `configurable_alternatives()` method allows us to swap out steps in a chain with an alternative. Below, we swap out one chat model for another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3db59f45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:10:38.634581Z",
     "iopub.status.busy": "2024-09-11T18:10:38.634295Z",
     "iopub.status.idle": "2024-09-11T18:10:39.841831Z",
     "shell.execute_reply": "2024-09-11T18:10:39.841516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "StdinNotImplementedError",
     "evalue": "getpass was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgetpass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getpass\n\u001b[0;32m----> 6\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANTHROPIC_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mgetpass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/langchain/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py:1256\u001b[0m, in \u001b[0;36mKernel.getpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1255\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetpass was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1258\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m: getpass was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet langchain-anthropic\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71248a9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:10:39.843388Z",
     "iopub.status.busy": "2024-09-11T18:10:39.843269Z",
     "iopub.status.idle": "2024-09-11T18:10:41.101890Z",
     "shell.execute_reply": "2024-09-11T18:10:41.100953Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here's a bear joke for you:\\n\\nWhy don't bears wear socks? \\nBecause they have bear feet!\\n\\nHow's that? I tried to come up with a silly pun involving bears. Hopefully you found it at least mildly amusing. Let me know if you'd like to hear another bear-themed joke.\", additional_kwargs={}, response_metadata={'id': 'msg_018ZbeSyjj9Er71NyZtBwLhp', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 72}}, id='run-f7c8c0f0-1e8a-4b44-a7ce-2e36f696333a-0', usage_metadata={'input_tokens': 13, 'output_tokens': 72, 'total_tokens': 85})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-haiku-20240307\", temperature=0\n",
    ").configurable_alternatives(\n",
    "    # This gives this field an id\n",
    "    # When configuring the end runnable, we can then use this id to configure this field\n",
    "    ConfigurableField(id=\"llm\"),\n",
    "    # This sets a default_key.\n",
    "    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used\n",
    "    default_key=\"anthropic\",\n",
    "    # This adds a new option, with name `openai` that is equal to `ChatOpenAI()`\n",
    "    openai=ChatOpenAI(),\n",
    "    # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model=\"gpt-4\")`\n",
    "    gpt4=ChatOpenAI(model=\"gpt-4\"),\n",
    "    # You can add more configuration options here\n",
    ")\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "chain = prompt | llm\n",
    "\n",
    "# By default it will call Anthropic\n",
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48b45337",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:10:41.106818Z",
     "iopub.status.busy": "2024-09-11T18:10:41.106522Z",
     "iopub.status.idle": "2024-09-11T18:10:41.850309Z",
     "shell.execute_reply": "2024-09-11T18:10:41.849553Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why did the bear break up with his girlfriend? \\n\\nBecause he couldn't bear the relationship any longer!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 13, 'total_tokens': 34}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-607f40c0-fecd-426b-abed-4860d432c8c5-0', usage_metadata={'input_tokens': 13, 'output_tokens': 21, 'total_tokens': 34})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use `.with_config(configurable={\"llm\": \"openai\"})` to specify an llm to use\n",
    "chain.with_config(configurable={\"llm\": \"openai\"}).invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42647fb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:10:41.855353Z",
     "iopub.status.busy": "2024-09-11T18:10:41.854326Z",
     "iopub.status.idle": "2024-09-11T18:10:43.247691Z",
     "shell.execute_reply": "2024-09-11T18:10:43.247126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here's a bear joke for you:\\n\\nWhy don't bears wear socks? \\nBecause they have bear feet!\\n\\nHow's that? I tried to come up with a silly pun involving bears. Hopefully you found it at least mildly amusing. Let me know if you'd like to hear another bear-themed joke.\", additional_kwargs={}, response_metadata={'id': 'msg_013S2ySwMd4uinEASJenB7Sh', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 72}}, id='run-72272a2a-2d1e-4ba1-bb2e-0d35b9d80ca9-0', usage_metadata={'input_tokens': 13, 'output_tokens': 72, 'total_tokens': 85})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we use the `default_key` then it uses the default\n",
    "chain.with_config(configurable={\"llm\": \"anthropic\"}).invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9134559",
   "metadata": {},
   "source": [
    "### With Prompts\n",
    "\n",
    "We can do a similar thing, but alternate between prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f6a7c6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:10:43.251953Z",
     "iopub.status.busy": "2024-09-11T18:10:43.251295Z",
     "iopub.status.idle": "2024-09-11T18:10:44.839531Z",
     "shell.execute_reply": "2024-09-11T18:10:44.838817Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here's a bear joke for you:\\n\\nWhy don't bears wear socks? \\nBecause they have bear feet!\\n\\nHow's that? I tried to come up with a silly pun involving bears. Hopefully you found it at least mildly amusing. Let me know if you'd like to hear another bear-themed joke.\", additional_kwargs={}, response_metadata={'id': 'msg_01SHjVVHx9aGZA5ryphMMDiK', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 72}}, id='run-e387e041-804f-4e22-823d-97d1faafd4b0-0', usage_metadata={'input_tokens': 13, 'output_tokens': 72, 'total_tokens': 85})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0)\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Tell me a joke about {topic}\"\n",
    ").configurable_alternatives(\n",
    "    # This gives this field an id\n",
    "    # When configuring the end runnable, we can then use this id to configure this field\n",
    "    ConfigurableField(id=\"prompt\"),\n",
    "    # This sets a default_key.\n",
    "    # If we specify this key, the default prompt (asking for a joke, as initialized above) will be used\n",
    "    default_key=\"joke\",\n",
    "    # This adds a new option, with name `poem`\n",
    "    poem=PromptTemplate.from_template(\"Write a short poem about {topic}\"),\n",
    "    # You can add more configuration options here\n",
    ")\n",
    "chain = prompt | llm\n",
    "\n",
    "# By default it will write a joke\n",
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "927297a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:10:44.842745Z",
     "iopub.status.busy": "2024-09-11T18:10:44.842486Z",
     "iopub.status.idle": "2024-09-11T18:10:46.876930Z",
     "shell.execute_reply": "2024-09-11T18:10:46.876206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here is a short poem about bears:\\n\\nMajestic bears, strong and true,\\nRoaming the forests, wild and free.\\nPowerful paws, fur soft and brown,\\nCommanding respect, a sight to see.\\n\\nForaging for berries, fishing streams,\\nProtecting their young, a fierce display.\\nSymbols of nature's untamed grace,\\nBears captivate us, day by day.\\n\\nMighty and gentle, bears inspire,\\nReminding us of Earth's precious might.\\nIn their presence, we stand in awe,\\nHumbled by their primal, noble plight.\", additional_kwargs={}, response_metadata={'id': 'msg_01HgwAsH1LH1eYfoEey7s1Bv', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 140}}, id='run-99848f28-e303-46d1-a5b0-77cae05f214b-0', usage_metadata={'input_tokens': 13, 'output_tokens': 140, 'total_tokens': 153})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can configure it write a poem\n",
    "chain.with_config(configurable={\"prompt\": \"poem\"}).invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c77124e",
   "metadata": {},
   "source": [
    "### With Prompts and LLMs\n",
    "\n",
    "We can also have multiple things configurable!\n",
    "Here's an example doing that with both prompts and LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97538c23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:10:46.881765Z",
     "iopub.status.busy": "2024-09-11T18:10:46.881427Z",
     "iopub.status.idle": "2024-09-11T18:10:49.034666Z",
     "shell.execute_reply": "2024-09-11T18:10:49.034140Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='In the forest deep and wild,\\nWhere the trees stand tall and proud,\\nRoams a creature fierce and mild,\\nThe bear, so strong and loud.\\n\\nWith fur as dark as night,\\nAnd eyes that gleam like gold,\\nThey roam the woods with might,\\nA sight to behold.\\n\\nWith a lumbering gait they tread,\\nThrough the undergrowth they roam,\\nTheir presence fills the forest with dread,\\nYet they are truly at home.\\n\\nSo let us admire these majestic beasts,\\nWith their power and their grace,\\nFor in the wild, they are the least,\\nYet in our hearts, they hold a place.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 124, 'prompt_tokens': 13, 'total_tokens': 137}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-decb0bbb-3cb2-47ed-8bf6-a18bbcaba4cd-0', usage_metadata={'input_tokens': 13, 'output_tokens': 124, 'total_tokens': 137})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-haiku-20240307\", temperature=0\n",
    ").configurable_alternatives(\n",
    "    # This gives this field an id\n",
    "    # When configuring the end runnable, we can then use this id to configure this field\n",
    "    ConfigurableField(id=\"llm\"),\n",
    "    # This sets a default_key.\n",
    "    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used\n",
    "    default_key=\"anthropic\",\n",
    "    # This adds a new option, with name `openai` that is equal to `ChatOpenAI()`\n",
    "    openai=ChatOpenAI(),\n",
    "    # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model=\"gpt-4\")`\n",
    "    gpt4=ChatOpenAI(model=\"gpt-4\"),\n",
    "    # You can add more configuration options here\n",
    ")\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Tell me a joke about {topic}\"\n",
    ").configurable_alternatives(\n",
    "    # This gives this field an id\n",
    "    # When configuring the end runnable, we can then use this id to configure this field\n",
    "    ConfigurableField(id=\"prompt\"),\n",
    "    # This sets a default_key.\n",
    "    # If we specify this key, the default prompt (asking for a joke, as initialized above) will be used\n",
    "    default_key=\"joke\",\n",
    "    # This adds a new option, with name `poem`\n",
    "    poem=PromptTemplate.from_template(\"Write a short poem about {topic}\"),\n",
    "    # You can add more configuration options here\n",
    ")\n",
    "chain = prompt | llm\n",
    "\n",
    "# We can configure it write a poem with OpenAI\n",
    "chain.with_config(configurable={\"prompt\": \"poem\", \"llm\": \"openai\"}).invoke(\n",
    "    {\"topic\": \"bears\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4ee9fbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:10:49.037769Z",
     "iopub.status.busy": "2024-09-11T18:10:49.037424Z",
     "iopub.status.idle": "2024-09-11T18:10:49.744530Z",
     "shell.execute_reply": "2024-09-11T18:10:49.744037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why don't bears like fast food? Because they can't catch it!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 13, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-661e5727-09f2-41cd-a1b0-be98a5ba943e-0', usage_metadata={'input_tokens': 13, 'output_tokens': 15, 'total_tokens': 28})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can always just configure only one if we want\n",
    "chain.with_config(configurable={\"llm\": \"openai\"}).invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fc4841",
   "metadata": {},
   "source": [
    "### Saving configurations\n",
    "\n",
    "We can also easily save configured chains as their own objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cf53202",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:10:49.747494Z",
     "iopub.status.busy": "2024-09-11T18:10:49.747253Z",
     "iopub.status.idle": "2024-09-11T18:10:50.292885Z",
     "shell.execute_reply": "2024-09-11T18:10:50.292379Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the bear dissolve in water?\\n\\nBecause it was polar!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 13, 'total_tokens': 26}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f1b068d2-ac7c-484a-a9c1-45b468cff548-0', usage_metadata={'input_tokens': 13, 'output_tokens': 13, 'total_tokens': 26})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_joke = chain.with_config(configurable={\"llm\": \"openai\"})\n",
    "\n",
    "openai_joke.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76702b0e",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "You now know how to configure a chain's internal steps at runtime.\n",
    "\n",
    "To learn more, see the other how-to guides on runnables in this section, including:\n",
    "\n",
    "- Using [.bind()](/docs/how_to/binding) as a simpler way to set a runnable's runtime parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43e3b70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
